{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#grupo","title":"Grupo","text":"<ol> <li>Pedro Ant\u00f4nio Silva</li> <li>Eric Andrei Lima Possato</li> <li>Gustavo Antony de Assis</li> </ol>"},{"location":"#projetos","title":"Projetos","text":"<ul> <li> Classification</li> <li> Regression</li> <li> Roteiro 3</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":""},{"location":"notebooks/processing/","title":"Notebook 1","text":"In\u00a0[1]: Copied! <pre>import kagglehub\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport os\n</pre> import kagglehub import pandas as pd from sklearn.preprocessing import StandardScaler import os <pre>c:\\Users\\gusta\\RedesNeurais\\neural-network-project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Download\npath = kagglehub.dataset_download(\"teejmahal20/airline-passenger-satisfaction\")\ndf = pd.read_csv(os.path.join(path, \"train.csv\"), index_col=0, header=0)\n</pre> # Download path = kagglehub.dataset_download(\"teejmahal20/airline-passenger-satisfaction\") df = pd.read_csv(os.path.join(path, \"train.csv\"), index_col=0, header=0) <pre>Downloading from https://www.kaggle.com/api/v1/datasets/download/teejmahal20/airline-passenger-satisfaction?dataset_version_number=1...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.71M/2.71M [00:01&lt;00:00, 2.39MB/s]</pre> <pre>Extracting files...\n</pre> <pre></pre> In\u00a0[3]: Copied! <pre>df.head()\n</pre> df.head() Out[3]: id Gender Customer Type Age Type of Travel Class Flight Distance Inflight wifi service Departure/Arrival time convenient Ease of Online booking ... Inflight entertainment On-board service Leg room service Baggage handling Checkin service Inflight service Cleanliness Departure Delay in Minutes Arrival Delay in Minutes satisfaction 0 70172 Male Loyal Customer 13 Personal Travel Eco Plus 460 3 4 3 ... 5 4 3 4 4 5 5 25 18.0 neutral or dissatisfied 1 5047 Male disloyal Customer 25 Business travel Business 235 3 2 3 ... 1 1 5 3 1 4 1 1 6.0 neutral or dissatisfied 2 110028 Female Loyal Customer 26 Business travel Business 1142 2 2 2 ... 5 4 3 4 4 4 5 0 0.0 satisfied 3 24026 Female Loyal Customer 25 Business travel Business 562 2 5 5 ... 2 2 5 3 1 4 2 11 9.0 neutral or dissatisfied 4 119299 Male Loyal Customer 61 Business travel Business 214 3 3 3 ... 3 3 4 4 3 3 3 0 0.0 satisfied <p>5 rows \u00d7 24 columns</p> In\u00a0[4]: Copied! <pre>#check nan values\nprint(df.isnull().sum())\n</pre> #check nan values print(df.isnull().sum()) <pre>id                                     0\nGender                                 0\nCustomer Type                          0\nAge                                    0\nType of Travel                         0\nClass                                  0\nFlight Distance                        0\nInflight wifi service                  0\nDeparture/Arrival time convenient      0\nEase of Online booking                 0\nGate location                          0\nFood and drink                         0\nOnline boarding                        0\nSeat comfort                           0\nInflight entertainment                 0\nOn-board service                       0\nLeg room service                       0\nBaggage handling                       0\nCheckin service                        0\nInflight service                       0\nCleanliness                            0\nDeparture Delay in Minutes             0\nArrival Delay in Minutes             310\nsatisfaction                           0\ndtype: int64\n</pre> In\u00a0[5]: Copied! <pre>#replace nan in \"Arrival Delay in Minutes\" with 0\ndf[\"Arrival Delay in Minutes\"].fillna(0, inplace=True)\n</pre> #replace nan in \"Arrival Delay in Minutes\" with 0 df[\"Arrival Delay in Minutes\"].fillna(0, inplace=True) <pre>C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_48100\\1611070765.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[\"Arrival Delay in Minutes\"].fillna(0, inplace=True)\n</pre> In\u00a0[6]: Copied! <pre>df['satisfaction'].value_counts()\n</pre> df['satisfaction'].value_counts() Out[6]: <pre>satisfaction\nneutral or dissatisfied    58879\nsatisfied                  45025\nName: count, dtype: int64</pre> In\u00a0[7]: Copied! <pre>categorical_vars = df.select_dtypes(include=['object']).columns.tolist()\nnumeric_vars = df.select_dtypes(include=['number']).columns.tolist()\n\nprint(\"Categorical variables:\", categorical_vars)\nprint(\"Numeric variables:\", numeric_vars)\n</pre> categorical_vars = df.select_dtypes(include=['object']).columns.tolist() numeric_vars = df.select_dtypes(include=['number']).columns.tolist()  print(\"Categorical variables:\", categorical_vars) print(\"Numeric variables:\", numeric_vars) <pre>Categorical variables: ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']\nNumeric variables: ['id', 'Age', 'Flight Distance', 'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking', 'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', 'Inflight entertainment', 'On-board service', 'Leg room service', 'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n</pre> In\u00a0[8]: Copied! <pre># process categorical variables to enter a model\ndf = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n</pre> # process categorical variables to enter a model df = pd.get_dummies(df, columns=categorical_vars, drop_first=True) In\u00a0[9]: Copied! <pre>df.head()\n</pre> df.head() Out[9]: id Age Flight Distance Inflight wifi service Departure/Arrival time convenient Ease of Online booking Gate location Food and drink Online boarding Seat comfort ... Inflight service Cleanliness Departure Delay in Minutes Arrival Delay in Minutes Gender_Male Customer Type_disloyal Customer Type of Travel_Personal Travel Class_Eco Class_Eco Plus satisfaction_satisfied 0 70172 13 460 3 4 3 1 5 3 5 ... 5 5 25 18.0 True False True False True False 1 5047 25 235 3 2 3 3 1 3 1 ... 4 1 1 6.0 True True False False False False 2 110028 26 1142 2 2 2 2 5 5 5 ... 4 5 0 0.0 False False False False False True 3 24026 25 562 2 5 5 5 2 2 2 ... 4 2 11 9.0 False False False False False False 4 119299 61 214 3 3 3 3 4 5 5 ... 3 3 0 0.0 True False False False False True <p>5 rows \u00d7 25 columns</p> In\u00a0[10]: Copied! <pre>scaler = StandardScaler()\ndf[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n</pre> scaler = StandardScaler() df[numeric_vars] = scaler.fit_transform(df[numeric_vars]) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[11]: Copied! <pre>df.to_csv(\"../data/processed_airline_passenger_satisfaction.csv\", index=False)\n</pre> df.to_csv(\"../data/processed_airline_passenger_satisfaction.csv\", index=False) In\u00a0[12]: Copied! <pre>#read csv\ndf = pd.read_csv(\"../data/processed_airline_passenger_satisfaction.csv\")\ndf.head()\n</pre> #read csv df = pd.read_csv(\"../data/processed_airline_passenger_satisfaction.csv\") df.head() Out[12]: id Age Flight Distance Inflight wifi service Departure/Arrival time convenient Ease of Online booking Gate location Food and drink Online boarding Seat comfort ... Inflight service Cleanliness Departure Delay in Minutes Arrival Delay in Minutes Gender_Male Customer Type_disloyal Customer Type of Travel_Personal Travel Class_Eco Class_Eco Plus satisfaction_satisfied 0 0.140077 -1.745279 -0.731539 0.203579 0.616172 0.173776 -1.547323 1.352264 -0.185532 1.183099 ... 1.156436 1.305870 0.266393 0.074169 True False True False True False 1 -1.598276 -0.951360 -0.957184 0.203579 -0.695245 0.173776 0.018094 -1.656326 -0.185532 -1.849315 ... 0.305848 -1.742292 -0.361375 -0.236313 True True False False False False 2 1.203935 -0.885200 -0.047584 -0.549533 -0.695245 -0.541060 -0.764614 1.352264 1.296496 1.183099 ... 0.305848 1.305870 -0.387532 -0.391554 False False False False False True 3 -1.091678 -0.951360 -0.629246 -0.549533 1.271880 1.603448 1.583511 -0.904178 -0.926545 -1.091211 ... 0.305848 -0.980251 -0.099805 -0.158692 False False False False False False 4 1.451402 1.430397 -0.978244 0.203579 -0.039537 0.173776 0.018094 0.600117 1.296496 1.183099 ... -0.544740 -0.218211 -0.387532 -0.391554 True False False False False True <p>5 rows \u00d7 25 columns</p>"},{"location":"notebooks/processing_regression/","title":"Processing regression","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport kagglehub\nimport os\n</pre> import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler import kagglehub import os <pre>c:\\Users\\gusta\\RedesNeurais\\neural-network-project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[3]: Copied! <pre>path = kagglehub.dataset_download(\"nehalbirla/vehicle-dataset-from-cardekho\")\ndf = pd.read_csv(os.path.join(path, \"CAR DETAILS FROM CAR DEKHO.csv\"), header=0)\nprint(path)\n</pre> path = kagglehub.dataset_download(\"nehalbirla/vehicle-dataset-from-cardekho\") df = pd.read_csv(os.path.join(path, \"CAR DETAILS FROM CAR DEKHO.csv\"), header=0) print(path) <pre>C:\\Users\\gusta\\.cache\\kagglehub\\datasets\\nehalbirla\\vehicle-dataset-from-cardekho\\versions\\4\n</pre> In\u00a0[4]: Copied! <pre>df.head()\n</pre> df.head() Out[4]: name year selling_price km_driven fuel seller_type transmission owner 0 Maruti 800 AC 2007 60000 70000 Petrol Individual Manual First Owner 1 Maruti Wagon R LXI Minor 2007 135000 50000 Petrol Individual Manual First Owner 2 Hyundai Verna 1.6 SX 2012 600000 100000 Diesel Individual Manual First Owner 3 Datsun RediGO T Option 2017 250000 46000 Petrol Individual Manual First Owner 4 Honda Amaze VX i-DTEC 2014 450000 141000 Diesel Individual Manual Second Owner In\u00a0[5]: Copied! <pre>TARGET = 'selling_price'\nCATEGORICAL_FEATURES = df.select_dtypes(include=['object']).columns.tolist()\nNUMERICAL_FEATURES = df.select_dtypes(include=['number']).columns.tolist()\n\ndf[f'{TARGET}_original'] = df[TARGET]\n\n\nprint(\"Categorical variables:\", CATEGORICAL_FEATURES)\nprint(\"Numeric variables:\", NUMERICAL_FEATURES)\n</pre> TARGET = 'selling_price' CATEGORICAL_FEATURES = df.select_dtypes(include=['object']).columns.tolist() NUMERICAL_FEATURES = df.select_dtypes(include=['number']).columns.tolist()  df[f'{TARGET}_original'] = df[TARGET]   print(\"Categorical variables:\", CATEGORICAL_FEATURES) print(\"Numeric variables:\", NUMERICAL_FEATURES) <pre>Categorical variables: ['name', 'fuel', 'seller_type', 'transmission', 'owner']\nNumeric variables: ['year', 'selling_price', 'km_driven']\n</pre> In\u00a0[6]: Copied! <pre>#replace 'name' column with 'brand' extracted from it to avoid high cardinality\ndf['brand'] = df['name'].str.split().str[0]\ndf.drop(columns=['name'], inplace=True)\n\n#update Categorical Features\nCATEGORICAL_FEATURES = df.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical variables:\", CATEGORICAL_FEATURES)\n</pre> #replace 'name' column with 'brand' extracted from it to avoid high cardinality df['brand'] = df['name'].str.split().str[0] df.drop(columns=['name'], inplace=True)  #update Categorical Features CATEGORICAL_FEATURES = df.select_dtypes(include=['object']).columns.tolist() print(\"Categorical variables:\", CATEGORICAL_FEATURES)  <pre>Categorical variables: ['fuel', 'seller_type', 'transmission', 'owner', 'brand']\n</pre> In\u00a0[7]: Copied! <pre>df.head()\n</pre> df.head() Out[7]: year selling_price km_driven fuel seller_type transmission owner selling_price_original brand 0 2007 60000 70000 Petrol Individual Manual First Owner 60000 Maruti 1 2007 135000 50000 Petrol Individual Manual First Owner 135000 Maruti 2 2012 600000 100000 Diesel Individual Manual First Owner 600000 Hyundai 3 2017 250000 46000 Petrol Individual Manual First Owner 250000 Datsun 4 2014 450000 141000 Diesel Individual Manual Second Owner 450000 Honda In\u00a0[8]: Copied! <pre>print(df['brand'].value_counts().head(20))\n\n# inspect missing values and shapes\nprint(\"Shape:\", df.shape)\nprint(df.isnull().sum())\n\n# look at target distribution\nprint(df['selling_price'].describe())\ndf['selling_price'].hist(bins=50)\n</pre> print(df['brand'].value_counts().head(20))  # inspect missing values and shapes print(\"Shape:\", df.shape) print(df.isnull().sum())  # look at target distribution print(df['selling_price'].describe()) df['selling_price'].hist(bins=50) <pre>brand\nMaruti           1280\nHyundai           821\nMahindra          365\nTata              361\nHonda             252\nFord              238\nToyota            206\nChevrolet         188\nRenault           146\nVolkswagen        107\nSkoda              68\nNissan             64\nAudi               60\nBMW                39\nFiat               37\nDatsun             37\nMercedes-Benz      35\nMitsubishi          6\nJaguar              6\nLand                5\nName: count, dtype: int64\nShape: (4340, 9)\nyear                      0\nselling_price             0\nkm_driven                 0\nfuel                      0\nseller_type               0\ntransmission              0\nowner                     0\nselling_price_original    0\nbrand                     0\ndtype: int64\ncount    4.340000e+03\nmean     5.041273e+05\nstd      5.785487e+05\nmin      2.000000e+04\n25%      2.087498e+05\n50%      3.500000e+05\n75%      6.000000e+05\nmax      8.900000e+06\nName: selling_price, dtype: float64\n</pre> Out[8]: <pre>&lt;Axes: &gt;</pre> In\u00a0[9]: Copied! <pre># Remove Duplicates\ninitial_rows = df.shape[0]\ndf.drop_duplicates(inplace=True)\n\n# Handle Missing Values\nif df.isnull().sum().any():\n    for col in NUMERICAL_FEATURES:\n        df[col].fillna(df[col].median(), inplace=True)\n    for col in CATEGORICAL_FEATURES:\n        df[col].fillna(df[col].mode()[0], inplace=True)\n</pre> # Remove Duplicates initial_rows = df.shape[0] df.drop_duplicates(inplace=True)  # Handle Missing Values if df.isnull().sum().any():     for col in NUMERICAL_FEATURES:         df[col].fillna(df[col].median(), inplace=True)     for col in CATEGORICAL_FEATURES:         df[col].fillna(df[col].mode()[0], inplace=True) In\u00a0[10]: Copied! <pre>before = df.shape[0]\n\nfor col in ['selling_price', 'km_driven']:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    print(f\"{col} | lower: {lower:.2f}, upper: {upper:.2f}\")\n    \n    # Filter out outliers\n    df = df[(df[col] &gt;= lower) &amp; (df[col] &lt;= upper)]\n\nafter = df.shape[0]\nprint(f\"Removed {before - after} outliers from the dataset.\")\n</pre> before = df.shape[0]  for col in ['selling_price', 'km_driven']:     Q1 = df[col].quantile(0.25)     Q3 = df[col].quantile(0.75)     IQR = Q3 - Q1     lower = Q1 - 1.5 * IQR     upper = Q3 + 1.5 * IQR     print(f\"{col} | lower: {lower:.2f}, upper: {upper:.2f}\")          # Filter out outliers     df = df[(df[col] &gt;= lower) &amp; (df[col] &lt;= upper)]  after = df.shape[0] print(f\"Removed {before - after} outliers from the dataset.\") <pre>selling_price | lower: -400000.00, upper: 1200000.00\nkm_driven | lower: -35507.50, upper: 165304.50\nRemoved 291 outliers from the dataset.\n</pre> In\u00a0[11]: Copied! <pre># confirm brand extraction worked\nprint(df['brand'].value_counts().head(20))\n\n# inspect missing values and shapes\nprint(\"Shape:\", df.shape)\nprint(df.isnull().sum())\n\n# look at target distribution\nprint(df['selling_price'].describe())\ndf['selling_price'].hist(bins=50)\n</pre> # confirm brand extraction worked print(df['brand'].value_counts().head(20))  # inspect missing values and shapes print(\"Shape:\", df.shape) print(df.isnull().sum())  # look at target distribution print(df['selling_price'].describe()) df['selling_price'].hist(bins=50)  <pre>brand\nMaruti           1040\nHyundai           613\nTata              288\nMahindra          269\nHonda             209\nFord              201\nChevrolet         147\nToyota            109\nRenault           106\nVolkswagen         90\nNissan             50\nSkoda              47\nFiat               31\nDatsun             29\nMercedes-Benz       8\nAudi                6\nBMW                 6\nMitsubishi          3\nAmbassador          3\nOpelCorsa           2\nName: count, dtype: int64\nShape: (3259, 9)\nyear                      0\nselling_price             0\nkm_driven                 0\nfuel                      0\nseller_type               0\ntransmission              0\nowner                     0\nselling_price_original    0\nbrand                     0\ndtype: int64\ncount    3.259000e+03\nmean     3.929040e+05\nstd      2.537223e+05\nmin      2.000000e+04\n25%      1.950000e+05\n50%      3.300000e+05\n75%      5.500000e+05\nmax      1.200000e+06\nName: selling_price, dtype: float64\n</pre> Out[11]: <pre>&lt;Axes: &gt;</pre> In\u00a0[12]: Copied! <pre># One-Hot Encoding for Categorical Features\ndf_encoded = pd.get_dummies(df, columns=CATEGORICAL_FEATURES, drop_first=True, dtype=int)\n\n# Identify the columns that need scaling (numerical, excluding the target)\nfeatures_to_scale = [col for col in df_encoded.columns if col in NUMERICAL_FEATURES]\n\n# Scale features to [-1, 1]\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\ndf_encoded[features_to_scale] = scaler.fit_transform(df_encoded[features_to_scale])\n</pre> # One-Hot Encoding for Categorical Features df_encoded = pd.get_dummies(df, columns=CATEGORICAL_FEATURES, drop_first=True, dtype=int)  # Identify the columns that need scaling (numerical, excluding the target) features_to_scale = [col for col in df_encoded.columns if col in NUMERICAL_FEATURES]  # Scale features to [-1, 1] scaler = MinMaxScaler(feature_range=(-1, 1))  df_encoded[features_to_scale] = scaler.fit_transform(df_encoded[features_to_scale]) In\u00a0[13]: Copied! <pre>df_encoded.to_csv(\"../data/processed_vehicles.csv\", index=False)\n</pre> df_encoded.to_csv(\"../data/processed_vehicles.csv\", index=False) In\u00a0[14]: Copied! <pre>df_encoded.head()\n</pre> df_encoded.head() Out[14]: year selling_price km_driven selling_price_original fuel_Diesel fuel_Electric fuel_LPG fuel_Petrol seller_type_Individual seller_type_Trustmark Dealer ... brand_Maruti brand_Mercedes-Benz brand_Mitsubishi brand_Nissan brand_OpelCorsa brand_Renault brand_Skoda brand_Tata brand_Toyota brand_Volkswagen 0 0.071429 -0.932203 -0.151522 60000 0 0 0 1 1 0 ... 1 0 0 0 0 0 0 0 0 0 1 0.071429 -0.805085 -0.393948 135000 0 0 0 1 1 0 ... 1 0 0 0 0 0 0 0 0 0 2 0.428571 -0.016949 0.212116 600000 1 0 0 0 1 0 ... 0 0 0 0 0 0 0 0 0 0 3 0.785714 -0.610169 -0.442433 250000 0 0 0 1 1 0 ... 0 0 0 0 0 0 0 0 0 0 4 0.571429 -0.271186 0.709089 450000 1 0 0 0 1 0 ... 0 0 0 0 0 0 0 0 0 0 <p>5 rows \u00d7 36 columns</p>"},{"location":"projetos/","title":"Index","text":"<p>Este compilado de documenta\u00e7\u00f5es se refere a projetos executados em 2025.2</p>"},{"location":"projetos/classification/","title":"Projeto 1","text":""},{"location":"projetos/classification/#report-airline-passenger-satisfaction-prediction","title":"Report: Airline Passenger Satisfaction Prediction","text":""},{"location":"projetos/classification/#1-dataset-selection","title":"1. Dataset Selection","text":""},{"location":"projetos/classification/#dataset-airline-passenger-satisfaction","title":"Dataset: Airline Passenger Satisfaction","text":"<p>Source: Kaggle</p> <p>URL: https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction</p> <p>Size: 129,880 passengers x 24 features.</p>"},{"location":"projetos/classification/#reason","title":"Reason:","text":"<p>This dataset presents a highly relevant business problem\u2014predicting customer satisfaction\u2014with sufficient size and complexity (22 input features) to make an MLP model meaningful.</p>"},{"location":"projetos/classification/#2-dataset-explanation","title":"2. Dataset Explanation","text":""},{"location":"projetos/classification/#21-overview-features","title":"2.1. Overview &amp; Features","text":"<p>The dataset contains survey results from airline passengers. The goal is a binary classification: predict if a passenger is \"satisfied\" or \"neutral or dissatisfied\".</p> <p>Target Variable: satisfaction (Categorical)</p> <p>Input Features:</p> <p>Customer &amp; Travel Context: Customer Type (Loyal/Disloyal), Type of Travel, Class, Flight Distance, Delay times.</p> <p>Service Ratings (Numerical, 1-5): Key features include Online boarding, Seat comfort, Inflight wifi/service, Food and drink, and On-board service.</p>"},{"location":"projetos/classification/#22-domain-context","title":"2.2. Domain Context","text":"<p>Understanding the drivers of passenger satisfaction is critical for customer retention and revenue in the competitive airline industry. This model can directly identify key service areas for improvement.</p>"},{"location":"projetos/classification/#23-potential-issues","title":"2.3. Potential Issues","text":"<p>Class Imbalance: The target is skewed (55% \"neutral/dissatisfied\", 45% \"satisfied\").</p> <p>Missing Values: A small number of missing values exist in the Arrival Delay column.</p> <p>Outliers: Numerical features like Delay and Flight Distance may have extreme values that need handling.</p>"},{"location":"projetos/classification/#3-data-cleaning","title":"3 Data Cleaning","text":""},{"location":"projetos/classification/#31-pre-processing","title":"3.1 Pre-Processing","text":"<p>The dataset underwent comprehensive pre-processing to ensure data quality and prepare it for neural network modeling:</p> <ul> <li> <p>Missing Value Handling:</p> <ul> <li>Identified 310 missing values in the Arrival Delay in Minutes column</li> </ul> </li> <li> <p>Implemented zero-imputation strategy, replacing missing values with 0, </p> <ul> <li>under the assumption that missing arrival delay data likely indicates minimal or no delay</li> </ul> </li> <li> <p>Categorical Variable Encoding:</p> <ul> <li> <p>Identified 5 categorical variables: Gender, Customer Type, Type of Travel, Class, and satisfaction</p> </li> <li> <p>Applied one-hot encoding using <code>pd.get_dummies()</code> with drop_first=True parameter     This transformed categorical features into binary dummy variables while avoiding multicollinearity</p> </li> </ul> </li> </ul>"},{"location":"projetos/classification/#32-feature-normalization-with-tanh-activation","title":"3.2 Feature Normalization with Tanh Activation","text":"<p>The numerical features were normalized using standardization to improve neural network performance:</p> <p>Applied StandardScaler from scikit-learn to all numerical variables, according to Transformation formula:</p> <p>z= ( x \u2212 \u03bc ) \u03c3 z=  \u03c3 (x\u2212\u03bc) \u200b This centers data around mean (\u03bc=0) with unit variance (\u03c3=1)</p>"},{"location":"projetos/classification/#benefits-for-neural-networks","title":"Benefits for neural networks:","text":"<ul> <li> <p>Accelerates convergence during training</p> </li> <li> <p>Prevents feature dominance due to different scales</p> </li> <li> <p>Improves gradient descent efficiency</p> </li> <li> <p>Tanh Normalization Compatibility</p> <ul> <li>The standardized features (range approximately [-3, +3]) are well-suited for tanh activation functions</li> <li>This normalization prevents saturation in tanh units during forward/backward propagation</li> </ul> </li> </ul> <p>The final processed dataset contains 25 features with normalized numerical variables and encoded categorical variables, ready for neural network training and evaluation.</p>"},{"location":"projetos/classification/#4-mlp-implementation","title":"4. MLP Implementation","text":"<p>First, we need to import the relevant libraries:</p> <ul> <li>Pandas to read the dataset</li> <li>Numpy to perform matrix operations</li> <li>MatPlotLib to generate graphs</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre> <p>Then, we need to define the important functions and derivatives: </p><pre><code># Tanh activation\ndef tanh(x):\n    return np.tanh(x)\n\n# Derivative of tanh with respect to pre-activation z\ndef tanh_derivative(z):\n    return 1.0 - np.tanh(z) ** 2\n\n# Sigmoid activation\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Binary cross-entropy loss (mean over batch/sample)\ndef binary_cross_entropy(y, y_hat):\n    eps = 1e-9\n    return -np.mean(y * np.log(y_hat + eps) + (1 - y) * np.log(1 - y_hat + eps))\n</code></pre><p></p> <p>In this case, Tanh is being used as the activation for the hidden layer, the fact that the data is normalized between -1 and 1 helps using this activation function. On the other hand, since we are dealing with a binary classification model, the output layer is using sigmoid, as a way to keep the results as 0 and 1.</p> <p>Following that, we can load the dataset and select the appropriate feature and target columns as well as define the hyparparemeters that are going to be used for the training.</p> <pre><code># Data loading and preparation\ncsv_path = \"./docs/data/processed_airline_passenger_satisfaction.csv\"\ndf = pd.read_csv(csv_path)\ntarget_col = \"satisfaction_satisfied\"\n\nif \"id\" in df.columns:\n    df = df.drop(columns=[\"id\"])\n\nfeature_cols = [c for c in df.columns if c != target_col]\n\nX = df[feature_cols].to_numpy(dtype=float)\ny = df[target_col].to_numpy()\ny = y.reshape(-1, 1)\n</code></pre> <pre><code># Hyperparameters\ninput_dim = X_train.shape[1]\nhidden_dim = 32\noutput_dim = 1\neta = 0.01\nepochs = 20\ninit_rng = np.random.RandomState(42)\n</code></pre>"},{"location":"projetos/classification/#5-model-training","title":"5. Model Training","text":"<p>Before training, we need to first split the dataset betwen train/test to allow for evaluation: </p><pre><code>rng = np.random.RandomState(42)\nperm = rng.permutation(X.shape[0])\nsplit = int(0.8 * X.shape[0])\ntrain_idx = perm[:split]\ntest_idx = perm[split:]\n\nX_train = X[train_idx]\ny_train = y[train_idx]\nX_test = X[test_idx]\ny_test = y[test_idx]\n</code></pre><p></p> <p>The parameter were initialized using an approximate method of the Xavier initialization that scales the weight with the number of inputs: </p><pre><code># Parameter initialization (Xavier for tanh)\nW1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim)\nb1 = np.zeros((hidden_dim, 1))\nW2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\nb2 = np.zeros((output_dim, 1))\n</code></pre><p></p> <p>This is the entire training script, I will then breakdown each section separately:</p> <pre><code>for epoch in range(epochs):\n    perm = rng.permutation(n_train)\n    X_shuffled = X_train[perm]\n    y_shuffled = y_train[perm]\n\n    total_loss = 0.0\n\n    for i in range(n_train):\n        x_i = X_shuffled[i].reshape(1, -1)\n        y_i = y_shuffled[i].reshape(1, 1)\n\n        Z1 = x_i.dot(W1.T) + b1.T\n        A1 = tanh(Z1)\n        Z2 = A1.dot(W2.T) + b2.T\n        A2 = sigmoid(Z2)\n\n        loss_i = binary_cross_entropy(y_i, A2)\n        total_loss += loss_i\n\n        dZ2 = A2 - y_i\n        dW2 = dZ2.T.dot(A1)\n        db2 = dZ2.T\n        dA1 = dZ2.dot(W2)\n        dZ1 = dA1 * tanh_derivative(Z1)\n        dW1 = dZ1.T.dot(x_i)\n        db1 = dZ1.T\n\n        W2 -= eta * dW2\n        b2 -= eta * db2\n        W1 -= eta * dW1\n        b1 -= eta * db1\n\n    avg_epoch_loss = total_loss / n_train\n    train_losses.append(avg_epoch_loss)\n\n    print(f\"Epoch {epoch+1}/{epochs}  Loss = {avg_epoch_loss:.6f}\")\n</code></pre>"},{"location":"projetos/classification/#51-forward-pass","title":"5.1. Forward pass","text":"<p>On the forward pass we perform 4 steps:  - Compute the weighted sum of inputs for each hidden neuron  - Apply the activation function (tanh) to the hidden layer   - Weighted sum of hidden activations for the single output neuron  - Convert the output layer into 0-1 probability (sigmoid)</p> <p>This is how those steps are performed in the code:  <code>Python Z1 = x_i.dot(W1.T) + b1.T A1 = tanh(Z1) Z2 = A1.dot(W2.T) + b2.T A2 = sigmoid(Z2)</code></p>"},{"location":"projetos/classification/#52-calculate-loss","title":"5.2. Calculate Loss","text":"<p>The loss is quickly calculated in the following code: </p><pre><code>loss_i = binary_cross_entropy(y_i, A2)\ntotal_loss += loss_i\n</code></pre> Since we are using SGD (calculating loss for each sample), the loss for each sample is added and then divided by the number of samples so we are left with the overall loss for the epoch.<p></p>"},{"location":"projetos/classification/#53-backwards-propagation","title":"5.3. Backwards Propagation","text":"<p>Now we propagate the errors backwards to allow further updating of the weights, this is performed in 6 steps:  - Output layer error  - Gradients for output layer weights and bias  - Backpropagate error into hidden layer  - Apply tanh derivative to get hidden layer error  - Gradients for hidden layer weights and bias  - Parameter updates (gradient descent)</p> <pre><code>dZ2 = A2 - y_i\ndW2 = dZ2.T.dot(A1)\ndb2 = dZ2.T\ndA1 = dZ2.dot(W2)\ndZ1 = dA1 * tanh_derivative(Z1)\ndW1 = dZ1.T.dot(x_i)\ndb1 = dZ1.T\n\nW2 -= eta * dW2\nb2 -= eta * db2\nW1 -= eta * dW1\nb1 -= eta * db1\n</code></pre> <p>At this point we have to factor in the average loss mentioned earlier: </p><pre><code>avg_epoch_loss = total_loss / n_train\ntrain_losses.append(avg_epoch_loss)\n</code></pre><p></p>"},{"location":"projetos/classification/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy","text":""},{"location":"projetos/classification/#61-data-splitting-and-validation","title":"6.1. Data Splitting and Validation","text":"<p>For this analysis, the complete dataset was divided into two distinct sets using an 80/20 ratio: 80% for the training set (<code>X_train</code>) and 20% for the test set (<code>X_test</code>).</p> <ul> <li>Training Set Size: 103,904 passengers  </li> <li>Test/Validation Set Size: 25,976 passengers  </li> </ul> <p>To ensure the reproducibility of the split, a fixed random state was utilized:</p> <pre><code>rng = np.random.RandomState(42)\n</code></pre> <p>In the absence of a separate dedicated validation set, the <code>X_test</code> set serves a dual purpose: - Used as the validation set during training for monitoring performance and guiding the early stopping mechanism. - Used as the final holdout set for evaluating the best-performing model parameters.</p> <p>This ensures the model\u2019s final evaluation is based on data it did not directly train on.</p>"},{"location":"projetos/classification/#62-training-mode","title":"6.2. Training Mode","text":"<p>The model was trained using Online Training, also known as Stochastic Gradient Descent (SGD) at the sample level.</p> <p>Rationale: The network parameters are updated after processing every single training sample (<code>(x_i, y_i)</code>). While computationally slower than batch training, this approach introduces high variance in gradient estimates, which can help the model escape shallow local minima early in training.  </p> <p>For this specific implementation, it maximizes the frequency of learning updates and clearly demonstrates the core principles of backpropagation on a single data point.</p>"},{"location":"projetos/classification/#63-overfitting-prevention-early-stopping","title":"6.3. Overfitting Prevention (Early Stopping)","text":"<p>To prevent the model from overfitting the training data\u2014a common issue when training for many epochs\u2014the Early Stopping technique was implemented.</p> <p>Logic (based on validation loss on <code>X_test</code>):</p> <ul> <li> <p>Monitoring:   The Binary Cross-Entropy Loss is calculated on the <code>X_test</code> set at the end of every epoch.</p> </li> <li> <p>Best Model Tracking:   Model parameters (<code>W\u2081</code>, <code>W\u2082</code>, <code>b\u2081</code>, <code>b\u2082</code>) are saved only if the validation loss improves (decreases) by at least   a specified minimum delta: <code>min_delta = 0.0001</code>.</p> </li> <li> <p>Patience:   A patience value of 5 epochs was set.   If the validation loss fails to meet the improvement threshold for 5 consecutive epochs, the training halts early,   and the best-saved parameters are restored for final evaluation.</p> </li> </ul> <p>This strategy ensures that the final reported metrics are derived from the model that demonstrated the best generalization performance on unseen data, rather than the model obtained at the end of the full 20 epochs.</p>"},{"location":"projetos/classification/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":""},{"location":"projetos/classification/#71-loss-curves-and-convergence-analysis","title":"7.1. Loss Curves and Convergence Analysis","text":"<p>The training process was visualized by tracking the Binary Cross-Entropy Loss for both the training and validation (test) sets across epochs.</p> <p>The trend shown in the loss curve plot (Figure 1: Loss vs. Epochs) demonstrates excellent model performance:</p> <ul> <li>Rapid Decrease: Both training and validation loss decrease rapidly within the first few epochs, confirming that the network is successfully learning the underlying patterns in the data.</li> <li>Convergence: The loss values begin to stabilize, or plateau, indicating the model is approaching a minimum in the error landscape.</li> <li>Early Stopping: The lowest validation loss (Test Loss = 0.103729) was achieved at Epoch 19. The training was halted shortly after this point due to the patience limit being reached, successfully preventing the model from continuing to learn noise in the training data, thereby avoiding overfitting and ensuring optimal generalization performance. The best model parameters were saved at this point.</li> </ul> <p>Code used to generate the loss curves:</p> <pre><code># 7. Error Curves and Visualization\nplt.figure(figsize=(10, 5))\n\n# Plot 1: Loss vs. Epochs\nplt.plot(train_losses[:len(train_losses)], label='Training Loss')\nplt.plot(test_losses[:len(test_losses)], label='Test (Validation) Loss')\nplt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Model ({best_epoch})')\nplt.title(\"Loss vs. Epochs (SGD per-sample with Early Stopping)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Binary Cross-Entropy Loss\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig('loss_curves.png') \nplt.close() \n</code></pre>"},{"location":"projetos/classification/#72-accuracy-curve","title":"7.2. Accuracy Curve","text":"<p>Although the primary early stopping decision was based on loss, tracking test accuracy further confirms the model's generalization ability, showing a steady increase and stabilization at a high level (Test Acc \u2248 0.9575 at the best epoch).</p> <p></p>"},{"location":"projetos/classification/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":"<p>The final model, utilizing the weights saved by the Early Stopping procedure (at Epoch 19), was evaluated on the held-out test set (20,781 samples).</p>"},{"location":"projetos/classification/#81-performance-metrics-table","title":"8.1. Performance Metrics Table","text":"<p>The core classification metrics were calculated to provide a complete picture of the model's effectiveness.</p> <p></p> <p>Discussion on Strengths: The model demonstrates high performance across all metrics, with an overall Accuracy of 95.71%. The ROC-AUC of 0.9928 is extremely high, indicating excellent separability between the two classes (Satisfied vs. Dissatisfied) across various probability thresholds. The high Precision (96.24%) suggests that when the model predicts a passenger is \"Satisfied\", it is correct the vast majority of the time, making its positive predictions highly reliable.</p> <p>Code for calculating the final metrics:</p> <pre><code># Final Forward Pass using best parameters\nA2_test = forward_pass(X_test, W1, b1, W2, b2)\ny_pred = (A2_test &gt; 0.5).astype(int)\n\n# 8. Evaluation Metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, A2_test)\n</code></pre>"},{"location":"projetos/classification/#82-confusion-matrix-analysis","title":"8.2. Confusion Matrix Analysis","text":"<p>The confusion matrix provides a detailed breakdown of correct and incorrect predictions, offering insight into the types of errors the model makes.</p> <p>Table 1: Confusion Matrix Results</p> <p></p> <p>Interpretation: - True Positives (8,324): The model correctly identified 8,324 satisfied passengers. - False Negatives (566): The model incorrectly classified 566 truly satisfied passengers as dissatisfied. This is the primary error type, representing satisfied customers who may be mistakenly flagged for unnecessary intervention or missed retention opportunities. - False Positives (325): The model incorrectly classified 325 truly dissatisfied passengers as satisfied. This is the least frequent error type, reflecting the model's high Precision.</p> <p>The combination of high Recall (93.63%) and very high Precision (96.24%) demonstrates that the MLP model is highly reliable and accurate in predicting passenger satisfaction across the test set.</p> <p>Code used to generate the confusion matrix heatmap:</p> <pre><code># Confusion Matrix calculation\ncm = confusion_matrix(y_test, y_pred)\n\n# Plot 2: Confusion Matrix Heatmap\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Predicted 0 (Dissatisfied)', 'Predicted 1 (Satisfied)'],\n            yticklabels=['Actual 0 (Dissatisfied)', 'Actual 1 (Satisfied)'])\nplt.title(\"Confusion Matrix (Early Stop Model)\")\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\nplt.close()\n</code></pre>"},{"location":"projetos/classification/MLP/","title":"MLP","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\nimport seaborn as sns \nimport os\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score import seaborn as sns  import os In\u00a0[\u00a0]: Copied! <pre># Tanh activation\ndef tanh(x):\n    return np.tanh(x)\n</pre> # Tanh activation def tanh(x):     return np.tanh(x) In\u00a0[\u00a0]: Copied! <pre># Derivative of tanh with respect to pre-activation z\ndef tanh_derivative(z):\n    return 1.0 - np.tanh(z) ** 2\n</pre> # Derivative of tanh with respect to pre-activation z def tanh_derivative(z):     return 1.0 - np.tanh(z) ** 2 In\u00a0[\u00a0]: Copied! <pre># Sigmoid activation\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n</pre> # Sigmoid activation def sigmoid(x):     return 1 / (1 + np.exp(-x)) In\u00a0[\u00a0]: Copied! <pre># Binary cross-entropy loss (mean over batch/sample)\ndef binary_cross_entropy(y, y_hat):\n    eps = 1.0e-9\n    # Ensure inputs are in the correct format for calculation\n    y = y.flatten()\n    y_hat = y_hat.flatten()\n    \n    # Prevents log(0) and log(1-0)\n    y_hat = np.clip(y_hat, eps, 1 - eps)\n    \n    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n</pre> # Binary cross-entropy loss (mean over batch/sample) def binary_cross_entropy(y, y_hat):     eps = 1.0e-9     # Ensure inputs are in the correct format for calculation     y = y.flatten()     y_hat = y_hat.flatten()          # Prevents log(0) and log(1-0)     y_hat = np.clip(y_hat, eps, 1 - eps)          return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) <p>--- Forward Pass and Loss Functions for Testing ---</p> In\u00a0[\u00a0]: Copied! <pre># Forward pass (used to calculate loss and predictions on test set)\ndef forward_pass(X, W1, b1, W2, b2):\n    # Transposition to align with the expected format of b1 and b2 (column-vectors)\n    Z1 = X.dot(W1.T) + b1.T\n    A1 = tanh(Z1)\n    Z2 = A1.dot(W2.T) + b2.T\n    A2 = sigmoid(Z2)\n    return A2\n</pre> # Forward pass (used to calculate loss and predictions on test set) def forward_pass(X, W1, b1, W2, b2):     # Transposition to align with the expected format of b1 and b2 (column-vectors)     Z1 = X.dot(W1.T) + b1.T     A1 = tanh(Z1)     Z2 = A1.dot(W2.T) + b2.T     A2 = sigmoid(Z2)     return A2 In\u00a0[\u00a0]: Copied! <pre># Calculation of loss on the dataset (X, y)\ndef calculate_loss(X, y, W1, b1, W2, b2):\n    A2 = forward_pass(X, W1, b1, W2, b2)\n    return binary_cross_entropy(y, A2)\n</pre> # Calculation of loss on the dataset (X, y) def calculate_loss(X, y, W1, b1, W2, b2):     A2 = forward_pass(X, W1, b1, W2, b2)     return binary_cross_entropy(y, A2) In\u00a0[\u00a0]: Copied! <pre># Data loading and preparation\nbase_path = os.path.dirname(os.path.abspath(__file__))\ncsv_path = os.path.join(base_path, '..', '..', 'data', 'processed_airline_passenger_satisfaction.csv')\ntry:\n    df = pd.read_csv(csv_path)\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {csv_path}. Using dummy data.\")\n    # Fallback: Create a dummy dataset if the file is not found\n    data = {'feature1': np.random.rand(100), 'feature2': np.random.rand(100), 'satisfaction_satisfied': np.random.randint(0, 2, 100)}\n    df = pd.DataFrame(data)\n</pre> # Data loading and preparation base_path = os.path.dirname(os.path.abspath(__file__)) csv_path = os.path.join(base_path, '..', '..', 'data', 'processed_airline_passenger_satisfaction.csv') try:     df = pd.read_csv(csv_path) except FileNotFoundError:     print(f\"Error: File not found at {csv_path}. Using dummy data.\")     # Fallback: Create a dummy dataset if the file is not found     data = {'feature1': np.random.rand(100), 'feature2': np.random.rand(100), 'satisfaction_satisfied': np.random.randint(0, 2, 100)}     df = pd.DataFrame(data) In\u00a0[\u00a0]: Copied! <pre>target_col = \"satisfaction_satisfied\"\n</pre> target_col = \"satisfaction_satisfied\" In\u00a0[\u00a0]: Copied! <pre>if \"id\" in df.columns:\n    df = df.drop(columns=[\"id\"])\n</pre> if \"id\" in df.columns:     df = df.drop(columns=[\"id\"]) In\u00a0[\u00a0]: Copied! <pre>feature_cols = [c for c in df.columns if c != target_col]\n</pre> feature_cols = [c for c in df.columns if c != target_col] In\u00a0[\u00a0]: Copied! <pre>X = df[feature_cols].to_numpy(dtype=float)\ny = df[target_col].to_numpy()\ny = y.reshape(-1, 1)\n</pre> X = df[feature_cols].to_numpy(dtype=float) y = df[target_col].to_numpy() y = y.reshape(-1, 1) In\u00a0[\u00a0]: Copied! <pre>rng = np.random.RandomState(42)\nperm = rng.permutation(X.shape[0])\nsplit = int(0.8 * X.shape[0])\ntrain_idx = perm[:split]\ntest_idx = perm[split:]\n</pre> rng = np.random.RandomState(42) perm = rng.permutation(X.shape[0]) split = int(0.8 * X.shape[0]) train_idx = perm[:split] test_idx = perm[split:] In\u00a0[\u00a0]: Copied! <pre>X_train = X[train_idx]\ny_train = y[train_idx]\nX_test = X[test_idx]\ny_test = y[test_idx]\n</pre> X_train = X[train_idx] y_train = y[train_idx] X_test = X[test_idx] y_test = y[test_idx] In\u00a0[\u00a0]: Copied! <pre>print(f\"Samples: total={X.shape[0]}, train={X_train.shape[0]}, test={X_test.shape[0]}\")\nprint(f\"Features used: {len(feature_cols)}\")\n</pre> print(f\"Samples: total={X.shape[0]}, train={X_train.shape[0]}, test={X_test.shape[0]}\") print(f\"Features used: {len(feature_cols)}\") In\u00a0[\u00a0]: Copied! <pre># Hyperparameters\ninput_dim = X_train.shape[1]\nhidden_dim = 32\noutput_dim = 1\neta = 0.01\nepochs = 20\ninit_rng = np.random.RandomState(42)\n</pre> # Hyperparameters input_dim = X_train.shape[1] hidden_dim = 32 output_dim = 1 eta = 0.01 epochs = 20 init_rng = np.random.RandomState(42) In\u00a0[\u00a0]: Copied! <pre># --- Early Stopping Hyperparameters ---\npatience = 5\nmin_delta = 0.0001\nbest_loss = np.inf\npatience_counter = 0\n</pre> # --- Early Stopping Hyperparameters --- patience = 5 min_delta = 0.0001 best_loss = np.inf patience_counter = 0 In\u00a0[\u00a0]: Copied! <pre># Parameter initialization (Xavier for tanh)\nW1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim)\nb1 = np.zeros((hidden_dim, 1))\nW2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\nb2 = np.zeros((output_dim, 1))\n</pre> # Parameter initialization (Xavier for tanh) W1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim) b1 = np.zeros((hidden_dim, 1)) W2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim) b2 = np.zeros((output_dim, 1)) In\u00a0[\u00a0]: Copied! <pre># --- Variables to store the BEST weights and bias (for Early Stopping) ---\nbest_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n</pre> # --- Variables to store the BEST weights and bias (for Early Stopping) --- best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy() In\u00a0[\u00a0]: Copied! <pre># Training loop using true SGD (update per sample)\nn_train = X_train.shape[0]\ntrain_losses = []\ntest_losses = [] # To store validation loss\ntest_accuracies = [] # To store validation accuracy per epoch\n</pre> # Training loop using true SGD (update per sample) n_train = X_train.shape[0] train_losses = [] test_losses = [] # To store validation loss test_accuracies = [] # To store validation accuracy per epoch In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- Starting SGD Training (with Early Stopping) ---\")\nfor epoch in range(epochs):\n    perm = rng.permutation(n_train)\n    X_shuffled = X_train[perm]\n    y_shuffled = y_train[perm]\n\n    total_train_loss = 0.0\n\n    # Training (Forward and Backward per sample)\n    for i in range(n_train):\n        x_i = X_shuffled[i].reshape(1, -1)\n        y_i = y_shuffled[i].reshape(1, 1)\n\n        # Forward Pass\n        Z1 = x_i.dot(W1.T) + b1.T\n        A1 = tanh(Z1)\n        Z2 = A1.dot(W2.T) + b2.T\n        A2 = sigmoid(Z2)\n\n        # Loss Calculation and Accumulation\n        loss_i = binary_cross_entropy(y_i, A2)\n        total_train_loss += loss_i\n\n        # Backward Pass (Gradient Calculation)\n        dZ2 = A2 - y_i \n        dW2 = dZ2.T.dot(A1)\n        db2 = dZ2.T\n        dA1 = dZ2.dot(W2)\n        dZ1 = dA1 * tanh_derivative(Z1)\n        dW1 = dZ1.T.dot(x_i)\n        db1 = dZ1.T\n\n        # Parameter Update (SGD)\n        W2 -= eta * dW2\n        b2 -= eta * db2\n        W1 -= eta * dW1\n        b1 -= eta * db1\n\n    # Epoch Loss Evaluation\n    avg_epoch_loss = total_train_loss / n_train\n    train_losses.append(avg_epoch_loss)\n\n    # Calculation of Loss and Accuracy on the Test Set (Validation)\n    A2_test_temp = forward_pass(X_test, W1, b1, W2, b2)\n    avg_test_loss = binary_cross_entropy(y_test, A2_test_temp)\n    test_losses.append(avg_test_loss)\n    \n    y_pred_test_temp = (A2_test_temp &gt; 0.5).astype(int)\n    accuracy_test_temp = accuracy_score(y_test, y_pred_test_temp)\n    test_accuracies.append(accuracy_test_temp)\n    \n    print(f\"Epoch {epoch+1}/{epochs} - Train Loss = {avg_epoch_loss:.6f} | Test Loss = {avg_test_loss:.6f} | Test Acc = {accuracy_test_temp:.4f}\")\n\n    # --- Early Stopping Check ---\n    if avg_test_loss &lt; best_loss - min_delta:\n        best_loss = avg_test_loss\n        patience_counter = 0\n        # Save the current best model weights\n        best_W1, best_b1 = W1.copy(), b1.copy()\n        best_W2, best_b2 = W2.copy(), b2.copy()\n        best_epoch = epoch + 1\n    else:\n        patience_counter += 1\n        if patience_counter &gt;= patience:\n            print(f\"\\nEarly stopping triggered after {epoch+1} epochs (Patience = {patience}). Best epoch was {best_epoch} with loss {best_loss:.6f}.\")\n            break\n</pre> print(\"\\n--- Starting SGD Training (with Early Stopping) ---\") for epoch in range(epochs):     perm = rng.permutation(n_train)     X_shuffled = X_train[perm]     y_shuffled = y_train[perm]      total_train_loss = 0.0      # Training (Forward and Backward per sample)     for i in range(n_train):         x_i = X_shuffled[i].reshape(1, -1)         y_i = y_shuffled[i].reshape(1, 1)          # Forward Pass         Z1 = x_i.dot(W1.T) + b1.T         A1 = tanh(Z1)         Z2 = A1.dot(W2.T) + b2.T         A2 = sigmoid(Z2)          # Loss Calculation and Accumulation         loss_i = binary_cross_entropy(y_i, A2)         total_train_loss += loss_i          # Backward Pass (Gradient Calculation)         dZ2 = A2 - y_i          dW2 = dZ2.T.dot(A1)         db2 = dZ2.T         dA1 = dZ2.dot(W2)         dZ1 = dA1 * tanh_derivative(Z1)         dW1 = dZ1.T.dot(x_i)         db1 = dZ1.T          # Parameter Update (SGD)         W2 -= eta * dW2         b2 -= eta * db2         W1 -= eta * dW1         b1 -= eta * db1      # Epoch Loss Evaluation     avg_epoch_loss = total_train_loss / n_train     train_losses.append(avg_epoch_loss)      # Calculation of Loss and Accuracy on the Test Set (Validation)     A2_test_temp = forward_pass(X_test, W1, b1, W2, b2)     avg_test_loss = binary_cross_entropy(y_test, A2_test_temp)     test_losses.append(avg_test_loss)          y_pred_test_temp = (A2_test_temp &gt; 0.5).astype(int)     accuracy_test_temp = accuracy_score(y_test, y_pred_test_temp)     test_accuracies.append(accuracy_test_temp)          print(f\"Epoch {epoch+1}/{epochs} - Train Loss = {avg_epoch_loss:.6f} | Test Loss = {avg_test_loss:.6f} | Test Acc = {accuracy_test_temp:.4f}\")      # --- Early Stopping Check ---     if avg_test_loss &lt; best_loss - min_delta:         best_loss = avg_test_loss         patience_counter = 0         # Save the current best model weights         best_W1, best_b1 = W1.copy(), b1.copy()         best_W2, best_b2 = W2.copy(), b2.copy()         best_epoch = epoch + 1     else:         patience_counter += 1         if patience_counter &gt;= patience:             print(f\"\\nEarly stopping triggered after {epoch+1} epochs (Patience = {patience}). Best epoch was {best_epoch} with loss {best_loss:.6f}.\")             break <p>--- End of Training and Final Evaluation ---</p> In\u00a0[\u00a0]: Copied! <pre># Use the best parameters found during training (saved before overfitting started)\nW1, b1, W2, b2 = best_W1, best_b1, best_W2, best_b2\n</pre> # Use the best parameters found during training (saved before overfitting started) W1, b1, W2, b2 = best_W1, best_b1, best_W2, best_b2 In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- Final Evaluation on the Test Set (Using Best Model) ---\")\n</pre> print(\"\\n--- Final Evaluation on the Test Set (Using Best Model) ---\") In\u00a0[\u00a0]: Copied! <pre># Final Forward Pass\nA2_test = forward_pass(X_test, W1, b1, W2, b2)\ny_pred = (A2_test &gt; 0.5).astype(int)\n</pre> # Final Forward Pass A2_test = forward_pass(X_test, W1, b1, W2, b2) y_pred = (A2_test &gt; 0.5).astype(int) In\u00a0[\u00a0]: Copied! <pre># 8. Evaluation Metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, A2_test) \n</pre> # 8. Evaluation Metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) roc_auc = roc_auc_score(y_test, A2_test)  In\u00a0[\u00a0]: Copied! <pre>print(\"-\" * 30)\nprint(f\"Final Test Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\nprint(\"-\" * 30)\n</pre> print(\"-\" * 30) print(f\"Final Test Accuracy: {accuracy:.4f}\") print(f\"Precision: {precision:.4f}\") print(f\"Recall: {recall:.4f}\") print(f\"F1-Score: {f1:.4f}\") print(f\"ROC-AUC: {roc_auc:.4f}\") print(\"-\" * 30) In\u00a0[\u00a0]: Copied! <pre># Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n</pre> # Confusion Matrix cm = confusion_matrix(y_test, y_pred) In\u00a0[\u00a0]: Copied! <pre># 7. Error Curves and Visualization\nplt.figure(figsize=(10, 5))\n</pre> # 7. Error Curves and Visualization plt.figure(figsize=(10, 5)) In\u00a0[\u00a0]: Copied! <pre># Plot 1: Loss vs. Epochs\n# We only plot up to the last executed epoch\nplt.plot(train_losses[:len(train_losses)], label='Training Loss')\nplt.plot(test_losses[:len(test_losses)], label='Test (Validation) Loss')\nplt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Model ({best_epoch})')\nplt.title(\"Loss vs. Epochs (SGD per-sample with Early Stopping)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Binary Cross-Entropy Loss\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig('loss_curves.png') \nplt.close() \n</pre> # Plot 1: Loss vs. Epochs # We only plot up to the last executed epoch plt.plot(train_losses[:len(train_losses)], label='Training Loss') plt.plot(test_losses[:len(test_losses)], label='Test (Validation) Loss') plt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Model ({best_epoch})') plt.title(\"Loss vs. Epochs (SGD per-sample with Early Stopping)\") plt.xlabel(\"Epoch\") plt.ylabel(\"Binary Cross-Entropy Loss\") plt.grid(True) plt.legend() plt.tight_layout() plt.savefig('loss_curves.png')  plt.close()  In\u00a0[\u00a0]: Copied! <pre># Plot 2: Confusion Matrix Heatmap\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Predicted 0 (Dissatisfied)', 'Predicted 1 (Satisfied)'],\n            yticklabels=['Actual 0 (Dissatisfied)', 'Actual 1 (Satisfied)'])\nplt.title(\"Confusion Matrix (Early Stop Model)\")\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\nplt.close()\n</pre> # Plot 2: Confusion Matrix Heatmap plt.figure(figsize=(6, 5)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,             xticklabels=['Predicted 0 (Dissatisfied)', 'Predicted 1 (Satisfied)'],             yticklabels=['Actual 0 (Dissatisfied)', 'Actual 1 (Satisfied)']) plt.title(\"Confusion Matrix (Early Stop Model)\") plt.ylabel(\"True Label\") plt.xlabel(\"Predicted Label\") plt.tight_layout() plt.savefig('confusion_matrix.png') plt.close() In\u00a0[\u00a0]: Copied! <pre># Plot 3: Final Metrics Table\nmetrics = {\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n    'Value': [f\"{accuracy:.4f}\", f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", f\"{roc_auc:.4f}\"]\n}\nmetrics_df = pd.DataFrame(metrics)\n</pre> # Plot 3: Final Metrics Table metrics = {     'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],     'Value': [f\"{accuracy:.4f}\", f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", f\"{roc_auc:.4f}\"] } metrics_df = pd.DataFrame(metrics) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(6, 2)) # Size adjustment for the table\nax.axis('off') # Removes axes\nax.axis('tight')\ntable = ax.table(cellText=metrics_df.values,\n                     colLabels=metrics_df.columns,\n                     cellLoc = 'center', \n                     loc = 'center',\n                     colColours=['#f3f3f3']*2)\ntable.auto_set_font_size(False)\ntable.set_fontsize(12)\ntable.scale(1, 1.5)\nplt.title(\"Final Classification Metrics (Early Stop Model)\")\nplt.savefig('metrics_table.png', bbox_inches='tight', pad_inches=0.1)\nplt.close()\n</pre> fig, ax = plt.subplots(figsize=(6, 2)) # Size adjustment for the table ax.axis('off') # Removes axes ax.axis('tight') table = ax.table(cellText=metrics_df.values,                      colLabels=metrics_df.columns,                      cellLoc = 'center',                       loc = 'center',                      colColours=['#f3f3f3']*2) table.auto_set_font_size(False) table.set_fontsize(12) table.scale(1, 1.5) plt.title(\"Final Classification Metrics (Early Stop Model)\") plt.savefig('metrics_table.png', bbox_inches='tight', pad_inches=0.1) plt.close() In\u00a0[\u00a0]: Copied! <pre>print(\"Plots saved (loss_curves.png, confusion_matrix.png, and metrics_table.png).\")\n</pre> print(\"Plots saved (loss_curves.png, confusion_matrix.png, and metrics_table.png).\")"},{"location":"projetos/regression/","title":"Projeto 2","text":""},{"location":"projetos/regression/#vehicle-price-data-preprocessing-eda","title":"Vehicle Price Data Preprocessing &amp; EDA","text":""},{"location":"projetos/regression/#1-dataset-selection-and-overview","title":"1. Dataset Selection and Overview","text":""},{"location":"projetos/regression/#11-dataset-information","title":"1.1. Dataset Information","text":"<p>Source: Kaggle</p> <p>URL: Vehicle Dataset from CarDekho</p> <p>Size: 8 Columns and 4340 Rows</p>"},{"location":"projetos/regression/#12-selection-reason","title":"1.2. Selection Reason","text":"<p>This dataset presents a practical regression problem - predicting vehicle selling prices - with rich features including both numerical and categorical data, making it ideal for demonstrating MLP regression capabilities in real-world scenarios.</p>"},{"location":"projetos/regression/#13-overview-features","title":"1.3. Overview &amp; Features","text":"<p>The dataset contains information about used cars and their selling prices. The goal is regression: predict the selling price of a vehicle based on its characteristics.</p> <p>Target Variable: selling_price (Numerical)</p> <p>Input Features:</p> <ul> <li>Vehicle Specifications: year, km_driven</li> <li>Technical Details: fuel type, seller_type, transmission</li> <li>Brand Information: extracted from vehicle name to represent manufacturer</li> <li>Sales Context: selling_price as target variable</li> </ul>"},{"location":"projetos/regression/#14-domain-context","title":"1.4. Domain Context","text":"<p>Accurate vehicle price prediction is essential in the automotive market, helping both dealers and customers make informed decisions. The model can identify key factors affecting a vehicle's resale value.</p>"},{"location":"projetos/regression/#15-potential-issues","title":"1.5. Potential Issues","text":"<ul> <li>Outliers: Features like 'km_driven' and 'selling_price' contain extreme values requiring treatment</li> <li>Brand Diversity: High cardinality in vehicle names/brands needs careful handling</li> <li>Feature Scaling: Wide range of values across different features (year vs. km_driven)</li> </ul>"},{"location":"projetos/regression/#2-data-processing-and-analysis","title":"2. Data Processing and Analysis","text":""},{"location":"projetos/regression/#21-data-loading-and-feature-definition","title":"2.1. Data Loading and Feature Definition","text":"<p>The dataset is loaded from KaggleHub, and features are automatically categorized based on their data types.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport kagglehub\nimport os\n\n# Load Data\npath = kagglehub.dataset_download(\"nehalbirla/vehicle-dataset-from-cardekho\")\ndf = pd.read_csv(os.path.join(path, \"CAR DETAILS FROM CAR DEKHO.csv\"), header=0)\n\n# Feature Definitions\nTARGET = 'selling_price'\nCATEGORICAL_FEATURES = df.select_dtypes(include=['object']).columns.tolist()\nNUMERICAL_FEATURES = df.select_dtypes(include=['number']).columns.tolist()\n\n# Save original target values\ndf[f'{TARGET}_original'] = df[TARGET]\n\n# Extract brand from name to reduce cardinality\ndf['brand'] = df['name'].str.split().str[0]\ndf.drop(columns=['name'], inplace=True)\n\n# Update categorical features after brand extraction\nCATEGORICAL_FEATURES = df.select_dtypes(include=['object']).columns.tolist()\n</code></pre>"},{"location":"projetos/regression/#22-exploratory-data-analysis-eda","title":"2.2. Exploratory Data Analysis (EDA)","text":"<p>Initial analysis focuses on data structure, missing values, and distribution of features.</p>"},{"location":"projetos/regression/#221-data-structure-and-missing-values","title":"2.2.1. Data Structure and Missing Values","text":"<p>Analysis of data completeness and feature distributions:</p> <pre><code># Inspect shapes and missing values\nprint(\"Shape:\", df.shape)\nprint(df.isnull().sum())\n\nShape: (4340, 9)\nyear                      0\nselling_price             0\nkm_driven                 0\nfuel                      0\nseller_type               0\ntransmission              0\nowner                     0\nselling_price_original    0\nbrand                     0\n</code></pre>"},{"location":"projetos/regression/#222-target-variable-analysis","title":"2.2.2. Target Variable Analysis","text":"<p>Examination of the selling price distribution:</p> <pre><code># Target variable statistics\nprint(df['selling_price'].describe())\n\ncount    4.340000e+03\nmean     5.041273e+05\nstd      5.785487e+05\nmin      2.000000e+04\n25%      2.087498e+05\n50%      3.500000e+05\n75%      6.000000e+05\nmax      8.900000e+06\nName: selling_price, dtype: float64\n\n# Histogram of selling prices\ndf['selling_price'].hist(bins=50)\n</code></pre> <p>Prices Histogram</p>"},{"location":"projetos/regression/#223-brand-distribution-analysis","title":"2.2.3. Brand Distribution Analysis","text":"<p>Analysis of vehicle brands in the dataset:</p> <pre><code># Top 20 brands by frequency\nprint(df['brand'].value_counts().head(20))\n\nbrand\nMaruti           1280\nHyundai           821\nMahindra          365\nTata              361\nHonda             252\nFord              238\nToyota            206\nChevrolet         188\nRenault           146\nVolkswagen        107\nSkoda              68\nNissan             64\nAudi               60\nBMW                39\nFiat               37\nDatsun             37\nMercedes-Benz      35\nMitsubishi          6\nJaguar              6\nLand                5\nName: count, dtype: int64\n</code></pre>"},{"location":"projetos/regression/#224-outlier-analysis","title":"2.2.4. Outlier Analysis","text":"<p>Analysis of numerical features for outlier detection using IQR method:</p> <pre><code># Outlier detection for key numerical features\nfor col in ['selling_price', 'km_driven']:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    print(f\"{col} | lower: {lower:.2f}, upper: {upper:.2f}\")\n\nselling_price | lower: -400000.00, upper: 1200000.00\nkm_driven | lower: -35507.50, upper: 165304.50\nRemoved 291 outliers from the dataset.\n</code></pre>"},{"location":"projetos/regression/#3-data-cleaning-and-feature-engineering","title":"3. Data Cleaning and Feature Engineering","text":"<p>The following steps are applied to prepare the data for consumption by a machine learning model.</p>"},{"location":"projetos/regression/#31-duplicates-missing-values-and-outlier-treatment","title":"3.1. Duplicates, Missing Values, and Outlier Treatment","text":"<ul> <li>Duplicate Removal: Redundant entries are removed to ensure data quality</li> <li>Missing Value Treatment: Check and handle any missing values in both numerical and categorical features</li> <li>Outlier Handling: IQR method used to detect and filter outliers in key numerical features (selling_price, km_driven)</li> </ul> <pre><code># Remove Duplicates\ninitial_rows = df.shape[0]\ndf.drop_duplicates(inplace=True)\n\n# Handle Missing Values\nif df.isnull().sum().any():\n    for col in NUMERICAL_FEATURES:\n        df[col].fillna(df[col].median(), inplace=True)\n    for col in CATEGORICAL_FEATURES:\n        df[col].fillna(df[col].mode()[0], inplace=True)\n\n# Filter Outliers using IQR method\nfor col in ['selling_price', 'km_driven']:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    df = df[(df[col] &gt;= lower) &amp; (df[col] &lt;= upper)]\n</code></pre>"},{"location":"projetos/regression/#32-feature-encoding-and-scaling","title":"3.2. Feature Encoding and Scaling","text":"<p>The final steps involve encoding categorical variables and scaling numerical features to a consistent range:</p> <pre><code># One-Hot Encoding for Categorical Features\ndf_encoded = pd.get_dummies(df, columns=CATEGORICAL_FEATURES, drop_first=True, dtype=int)\n\n# Identify features to scale (numerical features)\nfeatures_to_scale = [col for col in df_encoded.columns if col in NUMERICAL_FEATURES]\n\n# Scale features to [-1, 1] range\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndf_encoded[features_to_scale] = scaler.fit_transform(df_encoded[features_to_scale])\n\n# Save processed dataset\ndf_encoded.to_csv(\"../data/processed_vehicles.csv\", index=False)\n</code></pre>"},{"location":"projetos/regression/#4-mlp-implementation","title":"4. MLP Implementation","text":"<p>First, we need to import the relevant libraries:</p> <ul> <li>Pandas to read the dataset</li> <li>Numpy to perform matrix operations</li> <li>MatPlotLib to generate graphs</li> <li>Sklearn.metrics to calculate metrics of the regression model</li> <li></li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport os\n</code></pre> <p>Then, we need to define the important functions and derivatives: </p><pre><code># Tanh activation\ndef tanh(x):\n    return np.tanh(x)\n\n# Derivative of tanh with respect to pre-activation z\ndef tanh_derivative(z):\n    return 1.0 - np.tanh(z) ** 2\n\n# Regression loss per-sample 0.5*MSE and full-dataset MSE \ndef mse_loss(y_true, y_pred):\n    # y_true and y_pred are arrays shaped (n_samples, 1)\n    return np.mean((y_true - y_pred) ** 2)\n\ndef mse_loss_per_sample(y_true, y_pred):\n    # use 0.5*(y - yhat)^2 so derivative is (yhat - y)\n    return 0.5 * np.mean((y_true - y_pred) ** 2)\n</code></pre><p></p> <p>In this case, Tanh is being used as the activation for the hidden layer, the fact that the data is normalized between -1 and 1 helps using this activation function. Since this is a regression model, the output is linear, and has no activation. For this same reason, we are using MSE (mean squared error) to calculate the loss</p> <p>Following that, we can load the dataset and select the appropriate feature and target columns as well as define the hyparparemeters that are going to be used for the training.</p> <pre><code># Data loading and preparation\nbase_path = os.path.dirname(os.path.abspath(__file__))\ncsv_path = os.path.join(base_path, '..', '..', 'data', 'processed_vehicles.csv')\ntry:\n    df = pd.read_csv(csv_path)\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {csv_path}. Using dummy data.\")\n    data = {'feature1': np.random.rand(200), 'feature2': np.random.rand(200), 'selling_price': np.random.rand(200)*1e5}\n    df = pd.DataFrame(data)\n\n\nTARGET = 'selling_price'\nif TARGET not in df.columns:\n    raise ValueError(f\"Target column '{TARGET}' not found in DataFrame.\")\n\n# Drop id if present\nif \"id\" in df.columns:\n    df = df.drop(columns=[\"id\"])\n\n# Feature columns - drop any original/backup columns you don't want as features\nbackup_target_col = f\"{TARGET}_original\"\nfeature_cols = [c for c in df.columns if c not in (TARGET, backup_target_col)]\n\nX = df[feature_cols].to_numpy(dtype=float)\ny = df[[TARGET]].to_numpy(dtype=float)\n</code></pre> <pre><code># Hyperparameters\ninput_dim = X_train.shape[1]\nhidden_dim = 32\noutput_dim = 1\neta = 0.01\nepochs = 100\ninit_rng = np.random.RandomState(42)\n\n# Early stopping params\npatience = 10\nmin_delta = 1e-6\nbest_loss = np.inf\npatience_counter = 0\n</code></pre>"},{"location":"projetos/regression/#5-model-training","title":"5. Model Training","text":"<p>Before training, we need to first split the dataset betwen train/test to allow for evaluation: </p><pre><code>rng = np.random.RandomState(42)\nperm = rng.permutation(X.shape[0])\nsplit = int(0.8 * X.shape[0])\ntrain_idx = perm[:split]\ntest_idx = perm[split:]\n\nX_train = X[train_idx]\ny_train = y[train_idx]\nX_test = X[test_idx]\ny_test = y[test_idx]\n</code></pre><p></p> <p>The parameter were initialized using an approximate method of the Xavier initialization that scales the weight with the number of inputs: </p><pre><code># Parameter initialization (Xavier for tanh)\nW1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim)\nb1 = np.zeros((hidden_dim, 1))\nW2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\nb2 = np.zeros((output_dim, 1))\n\nbest_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n</code></pre><p></p> <p>This is the entire training script, I will then breakdown each section separately:</p> <pre><code>for epoch in range(epochs):\n    perm = rng.permutation(n_train)\n    X_shuffled = X_train[perm]\n    y_shuffled = y_train[perm]\n\n    total_train_loss = 0.0\n\n    # SGD per-sample\n    for i in range(n_train):\n        x_i = X_shuffled[i].reshape(1, -1)\n        y_i = y_shuffled[i].reshape(1, 1)\n\n        # Forward\n        Z1 = x_i.dot(W1.T) + b1.T\n        A1 = tanh(Z1)\n        Z2 = A1.dot(W2.T) + b2.T\n        A2 = Z2\n\n        # Loss (per-sample)\n        loss_i = mse_loss_per_sample(y_i, A2)\n        total_train_loss += loss_i\n\n        # Backpropagation\n        # dL/dA2 = A2 - y_i\n        dZ2 = (A2 - y_i)\n        dW2 = dZ2.T.dot(A1)\n        db2 = dZ2.T\n\n        dA1 = dZ2.dot(W2)\n        dZ1 = dA1 * tanh_derivative(Z1)\n        dW1 = dZ1.T.dot(x_i)\n        db1 = dZ1.T\n\n        # Parameter updates\n        W2 -= eta * dW2\n        b2 -= eta * db2\n        W1 -= eta * dW1\n        b1 -= eta * db1\n\n    avg_train_loss = total_train_loss / n_train\n    train_losses.append(avg_train_loss)\n\n    # Validation loss (full test set)\n    A2_val = forward_pass(X_test, W1, b1, W2, b2)\n    val_mse = mse_loss(y_test, A2_val)\n    val_losses.append(val_mse)\n\n    print(f\"Epoch {epoch+1}/{epochs} - Train Loss = {avg_train_loss:.8f} | Val MSE = {val_mse:.8f}\")\n</code></pre>"},{"location":"projetos/regression/#51-forward-pass","title":"5.1. Forward pass","text":"<p>On the forward pass we perform 4 steps:  - Compute the weighted sum of inputs for each hidden neuron  - Apply the activation function (tanh) to the hidden layer   - Weighted sum of hidden activations for the single output neuron  - Produce linear real-valued output (suitable for continuous target)</p> <p>This is how those steps are performed in the code:  ```Python</p>"},{"location":"projetos/regression/#forward","title":"Forward","text":"<p>Z1 = x_i.dot(W1.T) + b1.T A1 = tanh(Z1) Z2 = A1.dot(W2.T) + b2.T A2 = Z2  ```</p>"},{"location":"projetos/regression/#52-calculate-loss","title":"5.2. Calculate Loss","text":"<p>The loss is quickly calculated in the following code: </p><pre><code># Loss (per-sample)\nloss_i = mse_loss_per_sample(y_i, A2)\ntotal_train_loss += loss_i\n</code></pre> Since we are using SGD (calculating loss for each sample), the loss for each sample is added and then divided by the number of samples so we are left with the overall loss for the epoch.<p></p>"},{"location":"projetos/regression/#53-backwards-propagation","title":"5.3. Backwards Propagation","text":"<p>Now we propagate the errors backwards to allow further updating of the weights, this is performed in 6 steps:  - Output layer error  - Gradients for output layer weights and bias  - Backpropagate error into hidden layer  - Apply tanh derivative to get hidden layer error  - Gradients for hidden layer weights and bias  - Parameter updates (gradient descent)</p> <pre><code>dZ2 = A2 - y_i\ndW2 = dZ2.T.dot(A1)\ndb2 = dZ2.T\ndA1 = dZ2.dot(W2)\ndZ1 = dA1 * tanh_derivative(Z1)\ndW1 = dZ1.T.dot(x_i)\ndb1 = dZ1.T\n\nW2 -= eta * dW2\nb2 -= eta * db2\nW1 -= eta * dW1\nb1 -= eta * db1\n</code></pre> <p>At this point we have to factor in the average loss mentioned earlier: </p><pre><code>avg_train_loss = total_train_loss / n_train\ntrain_losses.append(avg_train_loss)\n</code></pre><p></p>"},{"location":"projetos/regression/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy","text":""},{"location":"projetos/regression/#61-data-splitting-approach","title":"6.1. Data Splitting Approach","text":"<p>The model employs a two-tier splitting strategy for robust evaluation:</p> <pre><code># Primary split: 85% train/validation, 15% test\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\n# Secondary split: 5-fold cross-validation on training data\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n</code></pre>"},{"location":"projetos/regression/#62-training-methodology","title":"6.2. Training Methodology","text":"<p>Mini-Batch Gradient Descent was selected for optimal balance between computational efficiency and convergence stability:</p> <pre><code>batch_size = 16  # Balance between stochastic noise and batch stability\n</code></pre> <p>Rationale: Mini-batch training provides smoother convergence than pure stochastic gradient descent while remaining more memory-efficient than full-batch training, making it suitable for medium-sized datasets.</p>"},{"location":"projetos/regression/#63-overfitting-prevention","title":"6.3. Overfitting Prevention","text":"<p>Multiple techniques combat overfitting:</p> <pre><code># Early stopping with patience mechanism\npatience = 10\nmin_delta = 1e-6\nbest_loss = np.inf\npatience_counter = 0\n\n# Cross-validation for robust performance estimation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n</code></pre> <p>Early Stopping Logic: Training halts if validation loss fails to improve by <code>min_delta</code> for <code>patience</code> consecutive epochs, preventing overfitting to training data.</p>"},{"location":"projetos/regression/#64-reproducibility-and-validation","title":"6.4. Reproducibility and Validation","text":"<pre><code>rng = np.random.RandomState(42)  # Fixed random seed\n</code></pre> <p>Validation Role: The holdout validation set monitors generalization performance during training and triggers early stopping, while k-fold cross-validation provides reliable performance estimates across different data partitions.</p>"},{"location":"projetos/regression/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":""},{"location":"projetos/regression/#71-convergence-monitoring","title":"7.1. Convergence Monitoring","text":"<p>Training and validation losses are tracked per epoch to analyze learning behavior:</p> <pre><code>train_losses = []\nval_losses = []\n\n# Record losses each epoch\ntrain_losses.append(avg_train_loss)\nval_losses.append(val_mse)\n</code></pre>"},{"location":"projetos/regression/#72-loss-curve-analysis","title":"7.2. Loss Curve Analysis","text":"<p>Multiple visualization approaches provide comprehensive insights:</p> <p>Individual Fold Analysis:  - Displays training and validation curves for all 5 folds - Reveals consistency (or variability) across different data splits - Identifies potential overfitting through train-val loss gaps</p> <p>Aggregate Performance:  - Averages validation losses across all folds - Shows overall model learning trend - Highlights convergence point and training stability</p>"},{"location":"projetos/regression/#73-interpretation-guidelines","title":"7.3. Interpretation Guidelines","text":"<p>Convergence Indicators: - Plateauing loss curves indicate model has learned available patterns - Consistent decrease suggests ongoing learning potential - Oscillating curves may indicate learning rate issues</p> <p>Overfitting Detection: - Growing gap between training and validation losses - Validation loss increasing while training loss decreases - Early stopping typically mitigates this risk</p>"},{"location":"projetos/regression/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":""},{"location":"projetos/regression/#81-comprehensive-regression-assessment","title":"8.1. Comprehensive Regression Assessment","text":"<p>The model employs multiple metrics to evaluate different aspects of performance:</p> <pre><code># Core regression metrics\nmse = mean_squared_error(y_test_original, y_pred_test_original)\nmae = mean_absolute_error(y_test_original, y_pred_test_original) \nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_original, y_pred_test_original)\n</code></pre>"},{"location":"projetos/regression/#82-baseline-comparison","title":"8.2. Baseline Comparison","text":"<p>Performance is contextualized against a simple mean predictor baseline:</p> <pre><code># Baseline model: predict mean of training data\nmean_baseline = np.mean(y_trainval_original)\ny_baseline = np.full_like(y_test_original, mean_baseline)\n\n# Calculate baseline metrics\nbaseline_mse = mean_squared_error(y_test_original, y_baseline)\nbaseline_r2 = r2_score(y_test_original, y_baseline)\n</code></pre>"},{"location":"projetos/regression/#83-results-interpretation","title":"8.3. Results Interpretation","text":"<p>Residual Analysis:  - Points randomly scattered around diagonal indicate good fit - Patterns suggest systematic prediction errors - Outliers visible as points far from equality line</p> <p>Metric-Specific Insights: - MSE/RMSE: Penalizes large errors, sensitive to outliers - MAE: More robust to outliers, interpretable in target units - R\u00b2: Proportion of variance explained, scale-independent</p> <p>Comparative Analysis:</p> <p>The model comparison results show the MLP significantly outperforms the baseline mean predictor:</p> Model MSE RMSE MAE R\u00b2 MLP 0.0102 0.1009 0.0687 0.9451 Baseline (Mean) 0.2127 0.4612 0.3434 -0.1475 <p>Note: The MLP model achieves excellent performance with R\u00b2 = 0.945, explaining 94.5% of the variance in vehicle prices, while the baseline model performs worse than simply predicting the mean (negative R\u00b2).</p>"},{"location":"projetos/regression/MLP_regression/","title":"MLP regression","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport os\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score import os In\u00a0[\u00a0]: Copied! <pre># Tanh activation\ndef tanh(x):\n    return np.tanh(x)\n</pre> # Tanh activation def tanh(x):     return np.tanh(x) In\u00a0[\u00a0]: Copied! <pre># Derivative of tanh with respect to pre-activation z\ndef tanh_derivative(z):\n    return 1.0 - np.tanh(z) ** 2\n</pre> # Derivative of tanh with respect to pre-activation z def tanh_derivative(z):     return 1.0 - np.tanh(z) ** 2 In\u00a0[\u00a0]: Copied! <pre># Regression loss per-sample 0.5*MSE and full-dataset MSE \ndef mse_loss(y_true, y_pred):\n    # y_true and y_pred are arrays shaped (n_samples, 1)\n    return np.mean((y_true - y_pred) ** 2)\n</pre> # Regression loss per-sample 0.5*MSE and full-dataset MSE  def mse_loss(y_true, y_pred):     # y_true and y_pred are arrays shaped (n_samples, 1)     return np.mean((y_true - y_pred) ** 2) In\u00a0[\u00a0]: Copied! <pre>def mse_loss_per_sample(y_true, y_pred):\n    # use 0.5*(y - yhat)^2 so derivative is (yhat - y)\n    return 0.5 * np.mean((y_true - y_pred) ** 2)\n</pre> def mse_loss_per_sample(y_true, y_pred):     # use 0.5*(y - yhat)^2 so derivative is (yhat - y)     return 0.5 * np.mean((y_true - y_pred) ** 2) In\u00a0[\u00a0]: Copied! <pre># Forward Pass and Loss Functions\ndef forward_pass(X, W1, b1, W2, b2):\n    # X: (n_samples, input_dim)\n    Z1 = X.dot(W1.T) + b1.T          \n    A1 = tanh(Z1)\n    Z2 = A1.dot(W2.T) + b2.T\n    A2 = Z2\n    return A2\n</pre> # Forward Pass and Loss Functions def forward_pass(X, W1, b1, W2, b2):     # X: (n_samples, input_dim)     Z1 = X.dot(W1.T) + b1.T               A1 = tanh(Z1)     Z2 = A1.dot(W2.T) + b2.T     A2 = Z2     return A2 In\u00a0[\u00a0]: Copied! <pre># Data loading and preparation\nbase_path = os.path.dirname(os.path.abspath(__file__))\ncsv_path = os.path.join(base_path, '..', '..', 'data', 'processed_vehicles.csv')\ntry:\n    df = pd.read_csv(csv_path)\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {csv_path}. Using dummy data.\")\n    data = {'feature1': np.random.rand(200), 'feature2': np.random.rand(200), 'selling_price': np.random.rand(200)*1e5}\n    df = pd.DataFrame(data)\n</pre> # Data loading and preparation base_path = os.path.dirname(os.path.abspath(__file__)) csv_path = os.path.join(base_path, '..', '..', 'data', 'processed_vehicles.csv') try:     df = pd.read_csv(csv_path) except FileNotFoundError:     print(f\"Error: File not found at {csv_path}. Using dummy data.\")     data = {'feature1': np.random.rand(200), 'feature2': np.random.rand(200), 'selling_price': np.random.rand(200)*1e5}     df = pd.DataFrame(data) In\u00a0[\u00a0]: Copied! <pre>TARGET = 'selling_price'\nif TARGET not in df.columns:\n    raise ValueError(f\"Target column '{TARGET}' not found in DataFrame.\")\n</pre> TARGET = 'selling_price' if TARGET not in df.columns:     raise ValueError(f\"Target column '{TARGET}' not found in DataFrame.\") In\u00a0[\u00a0]: Copied! <pre># Drop id if present\nif \"id\" in df.columns:\n    df = df.drop(columns=[\"id\"])\n</pre> # Drop id if present if \"id\" in df.columns:     df = df.drop(columns=[\"id\"]) In\u00a0[\u00a0]: Copied! <pre># Feature columns - drop any original/backup columns you don't want as features\nbackup_target_col = f\"{TARGET}_original\"\nfeature_cols = [c for c in df.columns if c not in (TARGET, backup_target_col)]\n</pre> # Feature columns - drop any original/backup columns you don't want as features backup_target_col = f\"{TARGET}_original\" feature_cols = [c for c in df.columns if c not in (TARGET, backup_target_col)] In\u00a0[\u00a0]: Copied! <pre>X = df[feature_cols].to_numpy(dtype=float)\ny = df[[TARGET]].to_numpy(dtype=float)\n</pre> X = df[feature_cols].to_numpy(dtype=float) y = df[[TARGET]].to_numpy(dtype=float) In\u00a0[\u00a0]: Copied! <pre># Shuffle and train/test split\nrng = np.random.RandomState(42)\nperm = rng.permutation(X.shape[0])\nsplit = int(0.8 * X.shape[0])\ntrain_idx = perm[:split]\ntest_idx = perm[split:]\n</pre> # Shuffle and train/test split rng = np.random.RandomState(42) perm = rng.permutation(X.shape[0]) split = int(0.8 * X.shape[0]) train_idx = perm[:split] test_idx = perm[split:] In\u00a0[\u00a0]: Copied! <pre>X_train = X[train_idx]\ny_train = y[train_idx]\nX_test = X[test_idx]\ny_test = y[test_idx]\n</pre> X_train = X[train_idx] y_train = y[train_idx] X_test = X[test_idx] y_test = y[test_idx] In\u00a0[\u00a0]: Copied! <pre>print(f\"Samples: total={X.shape[0]}, train={X_train.shape[0]}, test={X_test.shape[0]}\")\nprint(f\"Features used: {X.shape[1]}\")\n</pre> print(f\"Samples: total={X.shape[0]}, train={X_train.shape[0]}, test={X_test.shape[0]}\") print(f\"Features used: {X.shape[1]}\") In\u00a0[\u00a0]: Copied! <pre># Hyperparameters\ninput_dim = X_train.shape[1]\nhidden_dim = 32\noutput_dim = 1\neta = 0.01\nepochs = 100\ninit_rng = np.random.RandomState(42)\n</pre> # Hyperparameters input_dim = X_train.shape[1] hidden_dim = 32 output_dim = 1 eta = 0.01 epochs = 100 init_rng = np.random.RandomState(42) In\u00a0[\u00a0]: Copied! <pre># Early stopping params\npatience = 10\nmin_delta = 1e-6\nbest_loss = np.inf\npatience_counter = 0\n</pre> # Early stopping params patience = 10 min_delta = 1e-6 best_loss = np.inf patience_counter = 0 In\u00a0[\u00a0]: Copied! <pre># Parameter initialization (Xavier for tanh)\nW1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim)\nb1 = np.zeros((hidden_dim, 1))\nW2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\nb2 = np.zeros((output_dim, 1))\n</pre> # Parameter initialization (Xavier for tanh) W1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim) b1 = np.zeros((hidden_dim, 1)) W2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim) b2 = np.zeros((output_dim, 1)) In\u00a0[\u00a0]: Copied! <pre>best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n</pre> best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy() In\u00a0[\u00a0]: Copied! <pre>n_train = X_train.shape[0]\ntrain_losses = []\nval_losses = []\n</pre> n_train = X_train.shape[0] train_losses = [] val_losses = [] In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- Starting SGD Training (regression, per-sample updates) ---\")\nfor epoch in range(epochs):\n    perm = rng.permutation(n_train)\n    X_shuffled = X_train[perm]\n    y_shuffled = y_train[perm]\n\n    total_train_loss = 0.0\n\n    # SGD per-sample\n    for i in range(n_train):\n        x_i = X_shuffled[i].reshape(1, -1)\n        y_i = y_shuffled[i].reshape(1, 1)\n\n        # Forward\n        Z1 = x_i.dot(W1.T) + b1.T\n        A1 = tanh(Z1)\n        Z2 = A1.dot(W2.T) + b2.T\n        A2 = Z2\n\n        # Loss (per-sample)\n        loss_i = mse_loss_per_sample(y_i, A2)\n        total_train_loss += loss_i\n\n        # Backpropagation\n        # dL/dA2 = A2 - y_i\n        dZ2 = (A2 - y_i)\n        dW2 = dZ2.T.dot(A1)\n        db2 = dZ2.T\n\n        dA1 = dZ2.dot(W2)\n        dZ1 = dA1 * tanh_derivative(Z1)\n        dW1 = dZ1.T.dot(x_i)\n        db1 = dZ1.T\n\n        # Parameter updates\n        W2 -= eta * dW2\n        b2 -= eta * db2\n        W1 -= eta * dW1\n        b1 -= eta * db1\n\n    avg_train_loss = total_train_loss / n_train\n    train_losses.append(avg_train_loss)\n\n    # Validation loss (full test set)\n    A2_val = forward_pass(X_test, W1, b1, W2, b2)\n    val_mse = mse_loss(y_test, A2_val)\n    val_losses.append(val_mse)\n\n    print(f\"Epoch {epoch+1}/{epochs} - Train Loss = {avg_train_loss:.8f} | Val MSE = {val_mse:.8f}\")\n\n    # Early stopping\n    if val_mse &lt; best_loss - min_delta:\n        best_loss = val_mse\n        patience_counter = 0\n        best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n        best_epoch = epoch + 1\n    else:\n        patience_counter += 1\n        if patience_counter &gt;= patience:\n            print(f\"\\nEarly stopping triggered after {epoch+1} epochs. Best epoch = {best_epoch} with Val MSE = {best_loss:.8f}\")\n            break\n</pre> print(\"\\n--- Starting SGD Training (regression, per-sample updates) ---\") for epoch in range(epochs):     perm = rng.permutation(n_train)     X_shuffled = X_train[perm]     y_shuffled = y_train[perm]      total_train_loss = 0.0      # SGD per-sample     for i in range(n_train):         x_i = X_shuffled[i].reshape(1, -1)         y_i = y_shuffled[i].reshape(1, 1)          # Forward         Z1 = x_i.dot(W1.T) + b1.T         A1 = tanh(Z1)         Z2 = A1.dot(W2.T) + b2.T         A2 = Z2          # Loss (per-sample)         loss_i = mse_loss_per_sample(y_i, A2)         total_train_loss += loss_i          # Backpropagation         # dL/dA2 = A2 - y_i         dZ2 = (A2 - y_i)         dW2 = dZ2.T.dot(A1)         db2 = dZ2.T          dA1 = dZ2.dot(W2)         dZ1 = dA1 * tanh_derivative(Z1)         dW1 = dZ1.T.dot(x_i)         db1 = dZ1.T          # Parameter updates         W2 -= eta * dW2         b2 -= eta * db2         W1 -= eta * dW1         b1 -= eta * db1      avg_train_loss = total_train_loss / n_train     train_losses.append(avg_train_loss)      # Validation loss (full test set)     A2_val = forward_pass(X_test, W1, b1, W2, b2)     val_mse = mse_loss(y_test, A2_val)     val_losses.append(val_mse)      print(f\"Epoch {epoch+1}/{epochs} - Train Loss = {avg_train_loss:.8f} | Val MSE = {val_mse:.8f}\")      # Early stopping     if val_mse &lt; best_loss - min_delta:         best_loss = val_mse         patience_counter = 0         best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()         best_epoch = epoch + 1     else:         patience_counter += 1         if patience_counter &gt;= patience:             print(f\"\\nEarly stopping triggered after {epoch+1} epochs. Best epoch = {best_epoch} with Val MSE = {best_loss:.8f}\")             break In\u00a0[\u00a0]: Copied! <pre># Load best params\nW1, b1, W2, b2 = best_W1, best_b1, best_W2, best_b2\n</pre> # Load best params W1, b1, W2, b2 = best_W1, best_b1, best_W2, best_b2 In\u00a0[\u00a0]: Copied! <pre># Final predictions (on test set)\nA2_test = forward_pass(X_test, W1, b1, W2, b2)  # scaled/log target assumptions apply\ny_pred = A2_test.copy()\n</pre> # Final predictions (on test set) A2_test = forward_pass(X_test, W1, b1, W2, b2)  # scaled/log target assumptions apply y_pred = A2_test.copy() In\u00a0[\u00a0]: Copied! <pre>y_pred_orig = y_pred\ny_test_orig = y_test\n</pre> y_pred_orig = y_pred y_test_orig = y_test In\u00a0[\u00a0]: Copied! <pre># Regression metrics\nmse = mean_squared_error(y_test_orig, y_pred_orig)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test_orig, y_pred_orig)\nr2 = r2_score(y_test_orig, y_pred_orig)\n</pre> # Regression metrics mse = mean_squared_error(y_test_orig, y_pred_orig) rmse = np.sqrt(mse) mae = mean_absolute_error(y_test_orig, y_pred_orig) r2 = r2_score(y_test_orig, y_pred_orig) In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- Final Regression Metrics on Test Set ---\")\nprint(f\"MSE : {mse:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"MAE : {mae:.4f}\")\nprint(f\"R2  : {r2:.4f}\")\n</pre> print(\"\\n--- Final Regression Metrics on Test Set ---\") print(f\"MSE : {mse:.4f}\") print(f\"RMSE: {rmse:.4f}\") print(f\"MAE : {mae:.4f}\") print(f\"R2  : {r2:.4f}\") In\u00a0[\u00a0]: Copied! <pre># Plot: Loss curves\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss (per-sample 0.5*MSE)')\nplt.plot(val_losses, label='Validation MSE')\nplt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Epoch ({best_epoch})')\nplt.title(\"Loss vs Epochs (Regression)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig('loss_curves.png')\nplt.close()\n</pre> # Plot: Loss curves plt.figure(figsize=(10, 5)) plt.plot(train_losses, label='Training Loss (per-sample 0.5*MSE)') plt.plot(val_losses, label='Validation MSE') plt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Epoch ({best_epoch})') plt.title(\"Loss vs Epochs (Regression)\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.grid(True) plt.tight_layout() plt.savefig('loss_curves.png') plt.close() In\u00a0[\u00a0]: Copied! <pre># Scatter plot predicted vs actual\nplt.figure(figsize=(6,6))\nplt.scatter(y_test_orig, y_pred_orig, alpha=0.6)\nplt.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--')\nplt.xlabel('Actual Selling Price')\nplt.ylabel('Predicted Selling Price')\nplt.title('Predicted vs Actual (Test Set)')\nplt.tight_layout()\nplt.savefig('pred_vs_actual.png')\nplt.close()\n</pre> # Scatter plot predicted vs actual plt.figure(figsize=(6,6)) plt.scatter(y_test_orig, y_pred_orig, alpha=0.6) plt.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--') plt.xlabel('Actual Selling Price') plt.ylabel('Predicted Selling Price') plt.title('Predicted vs Actual (Test Set)') plt.tight_layout() plt.savefig('pred_vs_actual.png') plt.close() In\u00a0[\u00a0]: Copied! <pre>print(\"Plots saved: loss_curves.png, pred_vs_actual.png\")\n</pre> print(\"Plots saved: loss_curves.png, pred_vs_actual.png\")"},{"location":"projetos/regression/MLP_regression_K-fold/","title":"MLP regression K fold","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport os\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score from sklearn.model_selection import KFold, train_test_split from sklearn.preprocessing import StandardScaler import os In\u00a0[\u00a0]: Copied! <pre># Activation functions\ndef tanh(x): return np.tanh(x)\ndef tanh_derivative(z): return 1.0 - np.tanh(z) ** 2\ndef mse_loss(y_true, y_pred): return np.mean((y_true - y_pred) ** 2)\ndef mse_loss_per_sample(y_true, y_pred): return 0.5 * np.mean((y_true - y_pred) ** 2)\n</pre> # Activation functions def tanh(x): return np.tanh(x) def tanh_derivative(z): return 1.0 - np.tanh(z) ** 2 def mse_loss(y_true, y_pred): return np.mean((y_true - y_pred) ** 2) def mse_loss_per_sample(y_true, y_pred): return 0.5 * np.mean((y_true - y_pred) ** 2) In\u00a0[\u00a0]: Copied! <pre>def forward_pass(X, W1, b1, W2, b2):\n    Z1 = X.dot(W1.T) + b1.T\n    A1 = tanh(Z1)\n    Z2 = A1.dot(W2.T) + b2.T\n    return A1, Z2\n</pre> def forward_pass(X, W1, b1, W2, b2):     Z1 = X.dot(W1.T) + b1.T     A1 = tanh(Z1)     Z2 = A1.dot(W2.T) + b2.T     return A1, Z2 In\u00a0[\u00a0]: Copied! <pre># ---------------------- Data Loading ----------------------\nbase_path = os.path.dirname(os.path.abspath(__file__))\ncsv_path = os.path.join(base_path, '..', '..', 'data', 'processed_vehicles.csv')\ntry:\n    df = pd.read_csv(csv_path)\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {csv_path}. Using dummy data.\")\n    data = {'feature1': np.random.rand(200), 'feature2': np.random.rand(200), 'selling_price': np.random.rand(200)*1e5}\n    df = pd.DataFrame(data)\n</pre> # ---------------------- Data Loading ---------------------- base_path = os.path.dirname(os.path.abspath(__file__)) csv_path = os.path.join(base_path, '..', '..', 'data', 'processed_vehicles.csv') try:     df = pd.read_csv(csv_path) except FileNotFoundError:     print(f\"Error: File not found at {csv_path}. Using dummy data.\")     data = {'feature1': np.random.rand(200), 'feature2': np.random.rand(200), 'selling_price': np.random.rand(200)*1e5}     df = pd.DataFrame(data) In\u00a0[\u00a0]: Copied! <pre>TARGET = 'selling_price'\nif \"id\" in df.columns:\n    df = df.drop(columns=[\"id\"])\n</pre> TARGET = 'selling_price' if \"id\" in df.columns:     df = df.drop(columns=[\"id\"]) In\u00a0[\u00a0]: Copied! <pre>feature_cols = [c for c in df.columns if c != TARGET]\nX = df[feature_cols].to_numpy(dtype=float)\ny = df[[TARGET]].to_numpy(dtype=float)\n</pre> feature_cols = [c for c in df.columns if c != TARGET] X = df[feature_cols].to_numpy(dtype=float) y = df[[TARGET]].to_numpy(dtype=float) In\u00a0[\u00a0]: Copied! <pre># ---------------------- Split: 85% train/val + 15% test ----------------------\nrng = np.random.RandomState(42)\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\nprint(f\"Data split: train/val={X_trainval.shape[0]} | test={X_test.shape[0]}\")\n</pre> # ---------------------- Split: 85% train/val + 15% test ---------------------- rng = np.random.RandomState(42) X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42) print(f\"Data split: train/val={X_trainval.shape[0]} | test={X_test.shape[0]}\") In\u00a0[\u00a0]: Copied! <pre># ---------------------- Data Normalization ----------------------\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n</pre> # ---------------------- Data Normalization ---------------------- scaler_X = StandardScaler() scaler_y = StandardScaler() In\u00a0[\u00a0]: Copied! <pre>X_trainval = scaler_X.fit_transform(X_trainval)\nX_test = scaler_X.transform(X_test)\n</pre> X_trainval = scaler_X.fit_transform(X_trainval) X_test = scaler_X.transform(X_test) In\u00a0[\u00a0]: Copied! <pre>y_trainval = scaler_y.fit_transform(y_trainval)\ny_test = scaler_y.transform(y_test)\n</pre> y_trainval = scaler_y.fit_transform(y_trainval) y_test = scaler_y.transform(y_test) In\u00a0[\u00a0]: Copied! <pre># ---------------------- Baseline Model ----------------------\ny_trainval_original = scaler_y.inverse_transform(y_trainval)\nmean_baseline = np.mean(y_trainval_original)\ny_baseline = np.full_like(y_test, mean_baseline)\n</pre> # ---------------------- Baseline Model ---------------------- y_trainval_original = scaler_y.inverse_transform(y_trainval) mean_baseline = np.mean(y_trainval_original) y_baseline = np.full_like(y_test, mean_baseline) In\u00a0[\u00a0]: Copied! <pre># ---------------------- Hyperparameters ----------------------\ninput_dim = X.shape[1]\nhidden_dim = 32\noutput_dim = 1\neta = 0.001\nepochs = 100\nbatch_size = 16\npatience = 10\nmin_delta = 1e-6\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n</pre> # ---------------------- Hyperparameters ---------------------- input_dim = X.shape[1] hidden_dim = 32 output_dim = 1 eta = 0.001 epochs = 100 batch_size = 16 patience = 10 min_delta = 1e-6 kf = KFold(n_splits=5, shuffle=True, random_state=42) In\u00a0[\u00a0]: Copied! <pre># ---------------------- Output directory ----------------------\noutput_dir = os.path.join(base_path, 'K-fold')\nos.makedirs(output_dir, exist_ok=True)\n</pre> # ---------------------- Output directory ---------------------- output_dir = os.path.join(base_path, 'K-fold') os.makedirs(output_dir, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre># ---------------------- K-Fold Training ----------------------\nfold_results = []\nfold_idx = 1\nfold_histories = []\n</pre> # ---------------------- K-Fold Training ---------------------- fold_results = [] fold_idx = 1 fold_histories = [] In\u00a0[\u00a0]: Copied! <pre>for train_idx, val_idx in kf.split(X_trainval):\n    X_train, X_val = X_trainval[train_idx], X_trainval[val_idx]\n    y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]\n\n    init_rng = np.random.RandomState(42)\n    W1 = init_rng.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim)\n    b1 = np.zeros((hidden_dim, 1))\n    W2 = init_rng.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)\n    b2 = np.zeros((output_dim, 1))\n\n    best_loss = np.inf\n    patience_counter = 0\n    best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n\n    train_losses = []\n    val_losses = []\n\n    print(f\"\\n--- Fold {fold_idx} ---\")\n    for epoch in range(epochs):\n        perm = rng.permutation(X_train.shape[0])\n        X_train, y_train = X_train[perm], y_train[perm]\n        total_train_loss = 0.0\n        num_batches = 0\n\n        for start in range(0, X_train.shape[0], batch_size):\n            end = start + batch_size\n            xb, yb = X_train[start:end], y_train[start:end]\n            current_batch_size = xb.shape[0]\n\n            # Forward pass\n            A1, A2 = forward_pass(xb, W1, b1, W2, b2)\n            loss = mse_loss_per_sample(yb, A2)\n            total_train_loss += loss\n            num_batches += 1\n\n            # Backpropagation\n            dZ2 = (A2 - yb) / current_batch_size\n            dW2 = dZ2.T.dot(A1)\n            db2 = np.sum(dZ2, axis=0, keepdims=True).T\n            \n            dA1 = dZ2.dot(W2)\n            dZ1 = dA1 * tanh_derivative(A1)\n            dW1 = dZ1.T.dot(xb)\n            db1 = np.sum(dZ1, axis=0, keepdims=True).T\n\n            # Update weights\n            W1 -= eta * dW1\n            b1 -= eta * db1\n            W2 -= eta * dW2\n            b2 -= eta * db2\n\n        avg_train_loss = total_train_loss / num_batches\n        _, val_pred = forward_pass(X_val, W1, b1, W2, b2)\n        val_mse = mse_loss(y_val, val_pred)\n\n        train_losses.append(avg_train_loss)\n        val_losses.append(val_mse)\n\n        # Early stopping\n        if val_mse &lt; best_loss - min_delta:\n            best_loss = val_mse\n            best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= patience:\n                break\n\n    fold_histories.append((train_losses, val_losses))\n\n    # Evaluate fold\n    _, val_pred = forward_pass(X_val, best_W1, best_b1, best_W2, best_b2)\n    fold_mse = mean_squared_error(y_val, val_pred)\n    fold_mae = mean_absolute_error(y_val, val_pred)\n    fold_r2 = r2_score(y_val, val_pred)\n    fold_results.append((fold_mse, fold_mae, fold_r2))\n    fold_idx += 1\n</pre> for train_idx, val_idx in kf.split(X_trainval):     X_train, X_val = X_trainval[train_idx], X_trainval[val_idx]     y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]      init_rng = np.random.RandomState(42)     W1 = init_rng.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim)     b1 = np.zeros((hidden_dim, 1))     W2 = init_rng.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)     b2 = np.zeros((output_dim, 1))      best_loss = np.inf     patience_counter = 0     best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()      train_losses = []     val_losses = []      print(f\"\\n--- Fold {fold_idx} ---\")     for epoch in range(epochs):         perm = rng.permutation(X_train.shape[0])         X_train, y_train = X_train[perm], y_train[perm]         total_train_loss = 0.0         num_batches = 0          for start in range(0, X_train.shape[0], batch_size):             end = start + batch_size             xb, yb = X_train[start:end], y_train[start:end]             current_batch_size = xb.shape[0]              # Forward pass             A1, A2 = forward_pass(xb, W1, b1, W2, b2)             loss = mse_loss_per_sample(yb, A2)             total_train_loss += loss             num_batches += 1              # Backpropagation             dZ2 = (A2 - yb) / current_batch_size             dW2 = dZ2.T.dot(A1)             db2 = np.sum(dZ2, axis=0, keepdims=True).T                          dA1 = dZ2.dot(W2)             dZ1 = dA1 * tanh_derivative(A1)             dW1 = dZ1.T.dot(xb)             db1 = np.sum(dZ1, axis=0, keepdims=True).T              # Update weights             W1 -= eta * dW1             b1 -= eta * db1             W2 -= eta * dW2             b2 -= eta * db2          avg_train_loss = total_train_loss / num_batches         _, val_pred = forward_pass(X_val, W1, b1, W2, b2)         val_mse = mse_loss(y_val, val_pred)          train_losses.append(avg_train_loss)         val_losses.append(val_mse)          # Early stopping         if val_mse &lt; best_loss - min_delta:             best_loss = val_mse             best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()             patience_counter = 0         else:             patience_counter += 1             if patience_counter &gt;= patience:                 break      fold_histories.append((train_losses, val_losses))      # Evaluate fold     _, val_pred = forward_pass(X_val, best_W1, best_b1, best_W2, best_b2)     fold_mse = mean_squared_error(y_val, val_pred)     fold_mae = mean_absolute_error(y_val, val_pred)     fold_r2 = r2_score(y_val, val_pred)     fold_results.append((fold_mse, fold_mae, fold_r2))     fold_idx += 1 In\u00a0[\u00a0]: Copied! <pre># ---------------------- Loss Curves ----------------------\nplt.figure(figsize=(10, 6))\nfor i, (train_losses, val_losses) in enumerate(fold_histories, 1):\n    plt.plot(train_losses, label=f'Fold {i} Train Loss', linestyle='--')\n    plt.plot(val_losses, label=f'Fold {i} Val Loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss (MSE)\")\nplt.title(\"Training and Validation Loss per Fold\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, \"fold_loss_curves.png\"))\nplt.close()\n</pre> # ---------------------- Loss Curves ---------------------- plt.figure(figsize=(10, 6)) for i, (train_losses, val_losses) in enumerate(fold_histories, 1):     plt.plot(train_losses, label=f'Fold {i} Train Loss', linestyle='--')     plt.plot(val_losses, label=f'Fold {i} Val Loss') plt.xlabel(\"Epoch\") plt.ylabel(\"Loss (MSE)\") plt.title(\"Training and Validation Loss per Fold\") plt.legend() plt.grid(True) plt.tight_layout() plt.savefig(os.path.join(output_dir, \"fold_loss_curves.png\")) plt.close() In\u00a0[\u00a0]: Copied! <pre># Average validation curve\nval_curves = [np.array(v) for _, v in fold_histories]\nmax_len = max(len(v) for v in val_curves)\npadded_val_curves = np.array([np.pad(v, (0, max_len - len(v)), constant_values=np.nan) for v in val_curves])\navg_val_loss = np.nanmean(padded_val_curves, axis=0)\n</pre> # Average validation curve val_curves = [np.array(v) for _, v in fold_histories] max_len = max(len(v) for v in val_curves) padded_val_curves = np.array([np.pad(v, (0, max_len - len(v)), constant_values=np.nan) for v in val_curves]) avg_val_loss = np.nanmean(padded_val_curves, axis=0) In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 5))\nplt.plot(avg_val_loss, label='Average Validation Loss', color='blue')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Validation MSE\")\nplt.title(\"Average Validation Loss Across Folds\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, \"average_val_loss.png\"))\nplt.close()\n</pre> plt.figure(figsize=(8, 5)) plt.plot(avg_val_loss, label='Average Validation Loss', color='blue') plt.xlabel(\"Epoch\") plt.ylabel(\"Validation MSE\") plt.title(\"Average Validation Loss Across Folds\") plt.legend() plt.grid(True) plt.tight_layout() plt.savefig(os.path.join(output_dir, \"average_val_loss.png\")) plt.close() In\u00a0[\u00a0]: Copied! <pre># ---------------------- K-Fold Results ----------------------\navg_mse = np.mean([m for m, _, _ in fold_results])\navg_mae = np.mean([a for _, a, _ in fold_results])\navg_r2 = np.mean([r for _, _, r in fold_results])\n</pre> # ---------------------- K-Fold Results ---------------------- avg_mse = np.mean([m for m, _, _ in fold_results]) avg_mae = np.mean([a for _, a, _ in fold_results]) avg_r2 = np.mean([r for _, _, r in fold_results]) In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- K-Fold Validation Results ---\")\nprint(f\"Average Val MSE : {avg_mse:.4f}\")\nprint(f\"Average Val MAE : {avg_mae:.4f}\")\nprint(f\"Average Val R\u00b2  : {avg_r2:.4f}\")\n</pre> print(\"\\n--- K-Fold Validation Results ---\") print(f\"Average Val MSE : {avg_mse:.4f}\") print(f\"Average Val MAE : {avg_mae:.4f}\") print(f\"Average Val R\u00b2  : {avg_r2:.4f}\") In\u00a0[\u00a0]: Copied! <pre># ---------------------- Final Model ----------------------\ninit_rng = np.random.RandomState(42)\nW1_final = init_rng.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim)\nb1_final = np.zeros((hidden_dim, 1))\nW2_final = init_rng.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)\nb2_final = np.zeros((output_dim, 1))\n</pre> # ---------------------- Final Model ---------------------- init_rng = np.random.RandomState(42) W1_final = init_rng.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim) b1_final = np.zeros((hidden_dim, 1)) W2_final = init_rng.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim) b2_final = np.zeros((output_dim, 1)) In\u00a0[\u00a0]: Copied! <pre>for epoch in range(epochs):\n    perm = rng.permutation(X_trainval.shape[0])\n    X_trainval, y_trainval = X_trainval[perm], y_trainval[perm]\n    \n    for start in range(0, X_trainval.shape[0], batch_size):\n        end = start + batch_size\n        xb, yb = X_trainval[start:end], y_trainval[start:end]\n        current_batch_size = xb.shape[0]\n\n        A1, A2 = forward_pass(xb, W1_final, b1_final, W2_final, b2_final)\n        \n        dZ2 = (A2 - yb) / current_batch_size\n        dW2 = dZ2.T.dot(A1)\n        db2 = np.sum(dZ2, axis=0, keepdims=True).T\n        \n        dA1 = dZ2.dot(W2_final)\n        dZ1 = dA1 * tanh_derivative(A1)\n        dW1 = dZ1.T.dot(xb)\n        db1 = np.sum(dZ1, axis=0, keepdims=True).T\n\n        W1_final -= eta * dW1\n        b1_final -= eta * db1\n        W2_final -= eta * dW2\n        b2_final -= eta * db2\n</pre> for epoch in range(epochs):     perm = rng.permutation(X_trainval.shape[0])     X_trainval, y_trainval = X_trainval[perm], y_trainval[perm]          for start in range(0, X_trainval.shape[0], batch_size):         end = start + batch_size         xb, yb = X_trainval[start:end], y_trainval[start:end]         current_batch_size = xb.shape[0]          A1, A2 = forward_pass(xb, W1_final, b1_final, W2_final, b2_final)                  dZ2 = (A2 - yb) / current_batch_size         dW2 = dZ2.T.dot(A1)         db2 = np.sum(dZ2, axis=0, keepdims=True).T                  dA1 = dZ2.dot(W2_final)         dZ1 = dA1 * tanh_derivative(A1)         dW1 = dZ1.T.dot(xb)         db1 = np.sum(dZ1, axis=0, keepdims=True).T          W1_final -= eta * dW1         b1_final -= eta * db1         W2_final -= eta * dW2         b2_final -= eta * db2 In\u00a0[\u00a0]: Copied! <pre># ---------------------- Test Evaluation ----------------------\n_, y_pred_test = forward_pass(X_test, W1_final, b1_final, W2_final, b2_final)\n</pre> # ---------------------- Test Evaluation ---------------------- _, y_pred_test = forward_pass(X_test, W1_final, b1_final, W2_final, b2_final) In\u00a0[\u00a0]: Copied! <pre># Inverse transform for original scale\ny_pred_test_original = scaler_y.inverse_transform(y_pred_test)\ny_test_original = scaler_y.inverse_transform(y_test)\n</pre> # Inverse transform for original scale y_pred_test_original = scaler_y.inverse_transform(y_pred_test) y_test_original = scaler_y.inverse_transform(y_test) In\u00a0[\u00a0]: Copied! <pre># Calculate metrics\nmse = mean_squared_error(y_test_original, y_pred_test_original)\nmae = mean_absolute_error(y_test_original, y_pred_test_original)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_original, y_pred_test_original)\n</pre> # Calculate metrics mse = mean_squared_error(y_test_original, y_pred_test_original) mae = mean_absolute_error(y_test_original, y_pred_test_original) rmse = np.sqrt(mse) r2 = r2_score(y_test_original, y_pred_test_original) In\u00a0[\u00a0]: Copied! <pre># Baseline metrics\nbaseline_mse = mean_squared_error(y_test_original, scaler_y.inverse_transform(y_baseline))\nbaseline_mae = mean_absolute_error(y_test_original, scaler_y.inverse_transform(y_baseline))\nbaseline_rmse = np.sqrt(baseline_mse)\nbaseline_r2 = r2_score(y_test_original, scaler_y.inverse_transform(y_baseline))\n</pre> # Baseline metrics baseline_mse = mean_squared_error(y_test_original, scaler_y.inverse_transform(y_baseline)) baseline_mae = mean_absolute_error(y_test_original, scaler_y.inverse_transform(y_baseline)) baseline_rmse = np.sqrt(baseline_mse) baseline_r2 = r2_score(y_test_original, scaler_y.inverse_transform(y_baseline)) In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- Final Test Performance ---\")\nprint(f\"MSE  : {mse:.4f}\")\nprint(f\"RMSE : {rmse:.4f}\")\nprint(f\"MAE  : {mae:.4f}\")\nprint(f\"R\u00b2   : {r2:.4f}\")\n</pre> print(\"\\n--- Final Test Performance ---\") print(f\"MSE  : {mse:.4f}\") print(f\"RMSE : {rmse:.4f}\") print(f\"MAE  : {mae:.4f}\") print(f\"R\u00b2   : {r2:.4f}\") In\u00a0[\u00a0]: Copied! <pre># ---------------------- Results DataFrame ----------------------\nresults_df = pd.DataFrame({\n    'Model': ['MLP', 'Baseline (Mean)'],\n    'MSE': [mse, baseline_mse],\n    'RMSE': [rmse, baseline_rmse],\n    'MAE': [mae, baseline_mae],\n    'R2': [r2, baseline_r2]\n})\n</pre> # ---------------------- Results DataFrame ---------------------- results_df = pd.DataFrame({     'Model': ['MLP', 'Baseline (Mean)'],     'MSE': [mse, baseline_mse],     'RMSE': [rmse, baseline_rmse],     'MAE': [mae, baseline_mae],     'R2': [r2, baseline_r2] }) In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- Model Comparison ---\")\nprint(results_df.to_string(index=False))\n</pre> print(\"\\n--- Model Comparison ---\") print(results_df.to_string(index=False)) In\u00a0[\u00a0]: Copied! <pre># Save results\nresults_df.to_csv(os.path.join(output_dir, \"model_comparison.csv\"), index=False)\n</pre> # Save results results_df.to_csv(os.path.join(output_dir, \"model_comparison.csv\"), index=False) In\u00a0[\u00a0]: Copied! <pre># ---------------------- Residual Plot ----------------------\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test_original, y_pred_test_original, alpha=0.6)\nplt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--')\nplt.xlabel(\"Actual Selling Price\")\nplt.ylabel(\"Predicted Selling Price\")\nplt.title(\"Predicted vs Actual (Test Set)\")\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, \"residual_plot.png\"))\nplt.close()\n</pre> # ---------------------- Residual Plot ---------------------- plt.figure(figsize=(6, 6)) plt.scatter(y_test_original, y_pred_test_original, alpha=0.6) plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--') plt.xlabel(\"Actual Selling Price\") plt.ylabel(\"Predicted Selling Price\") plt.title(\"Predicted vs Actual (Test Set)\") plt.grid(True) plt.tight_layout() plt.savefig(os.path.join(output_dir, \"residual_plot.png\")) plt.close() In\u00a0[\u00a0]: Copied! <pre># Save metrics\nmetrics = pd.DataFrame({\n    \"MSE\": [mse],\n    \"RMSE\": [rmse],\n    \"MAE\": [mae],\n    \"R2\": [r2]\n})\nmetrics.to_csv(os.path.join(output_dir, \"final_metrics.csv\"), index=False)\n</pre> # Save metrics metrics = pd.DataFrame({     \"MSE\": [mse],     \"RMSE\": [rmse],     \"MAE\": [mae],     \"R2\": [r2] }) metrics.to_csv(os.path.join(output_dir, \"final_metrics.csv\"), index=False) In\u00a0[\u00a0]: Copied! <pre>print(f\"\\nPlots and metrics saved in: {output_dir}\")\n</pre> print(f\"\\nPlots and metrics saved in: {output_dir}\")"},{"location":"thisdocumentation/","title":"This documentation","text":""},{"location":"thisdocumentation/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}