{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#grupo","title":"Grupo","text":"<ol> <li>Pedro Ant\u00f4nio Silva</li> <li>Eric Andrei Lima Possato</li> <li>Gustavo Antony de Assis</li> </ol>"},{"location":"#projetos","title":"Projetos","text":"<ul> <li> Classification</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":""},{"location":"notebooks/processing/","title":"Notebook 1","text":"In\u00a0[1]: Copied! <pre>import kagglehub\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport os\n</pre> import kagglehub import pandas as pd from sklearn.preprocessing import StandardScaler import os <pre>c:\\Users\\gusta\\RedesNeurais\\neural-network-project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Download\npath = kagglehub.dataset_download(\"teejmahal20/airline-passenger-satisfaction\")\ndf = pd.read_csv(os.path.join(path, \"train.csv\"), index_col=0, header=0)\n</pre> # Download path = kagglehub.dataset_download(\"teejmahal20/airline-passenger-satisfaction\") df = pd.read_csv(os.path.join(path, \"train.csv\"), index_col=0, header=0) <pre>Downloading from https://www.kaggle.com/api/v1/datasets/download/teejmahal20/airline-passenger-satisfaction?dataset_version_number=1...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.71M/2.71M [00:01&lt;00:00, 2.39MB/s]</pre> <pre>Extracting files...\n</pre> <pre></pre> In\u00a0[3]: Copied! <pre>df.head()\n</pre> df.head() Out[3]: id Gender Customer Type Age Type of Travel Class Flight Distance Inflight wifi service Departure/Arrival time convenient Ease of Online booking ... Inflight entertainment On-board service Leg room service Baggage handling Checkin service Inflight service Cleanliness Departure Delay in Minutes Arrival Delay in Minutes satisfaction 0 70172 Male Loyal Customer 13 Personal Travel Eco Plus 460 3 4 3 ... 5 4 3 4 4 5 5 25 18.0 neutral or dissatisfied 1 5047 Male disloyal Customer 25 Business travel Business 235 3 2 3 ... 1 1 5 3 1 4 1 1 6.0 neutral or dissatisfied 2 110028 Female Loyal Customer 26 Business travel Business 1142 2 2 2 ... 5 4 3 4 4 4 5 0 0.0 satisfied 3 24026 Female Loyal Customer 25 Business travel Business 562 2 5 5 ... 2 2 5 3 1 4 2 11 9.0 neutral or dissatisfied 4 119299 Male Loyal Customer 61 Business travel Business 214 3 3 3 ... 3 3 4 4 3 3 3 0 0.0 satisfied <p>5 rows \u00d7 24 columns</p> In\u00a0[4]: Copied! <pre>#check nan values\nprint(df.isnull().sum())\n</pre> #check nan values print(df.isnull().sum()) <pre>id                                     0\nGender                                 0\nCustomer Type                          0\nAge                                    0\nType of Travel                         0\nClass                                  0\nFlight Distance                        0\nInflight wifi service                  0\nDeparture/Arrival time convenient      0\nEase of Online booking                 0\nGate location                          0\nFood and drink                         0\nOnline boarding                        0\nSeat comfort                           0\nInflight entertainment                 0\nOn-board service                       0\nLeg room service                       0\nBaggage handling                       0\nCheckin service                        0\nInflight service                       0\nCleanliness                            0\nDeparture Delay in Minutes             0\nArrival Delay in Minutes             310\nsatisfaction                           0\ndtype: int64\n</pre> In\u00a0[5]: Copied! <pre>#replace nan in \"Arrival Delay in Minutes\" with 0\ndf[\"Arrival Delay in Minutes\"].fillna(0, inplace=True)\n</pre> #replace nan in \"Arrival Delay in Minutes\" with 0 df[\"Arrival Delay in Minutes\"].fillna(0, inplace=True) <pre>C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_48100\\1611070765.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[\"Arrival Delay in Minutes\"].fillna(0, inplace=True)\n</pre> In\u00a0[6]: Copied! <pre>df['satisfaction'].value_counts()\n</pre> df['satisfaction'].value_counts() Out[6]: <pre>satisfaction\nneutral or dissatisfied    58879\nsatisfied                  45025\nName: count, dtype: int64</pre> In\u00a0[7]: Copied! <pre>categorical_vars = df.select_dtypes(include=['object']).columns.tolist()\nnumeric_vars = df.select_dtypes(include=['number']).columns.tolist()\n\nprint(\"Categorical variables:\", categorical_vars)\nprint(\"Numeric variables:\", numeric_vars)\n</pre> categorical_vars = df.select_dtypes(include=['object']).columns.tolist() numeric_vars = df.select_dtypes(include=['number']).columns.tolist()  print(\"Categorical variables:\", categorical_vars) print(\"Numeric variables:\", numeric_vars) <pre>Categorical variables: ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']\nNumeric variables: ['id', 'Age', 'Flight Distance', 'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking', 'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', 'Inflight entertainment', 'On-board service', 'Leg room service', 'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n</pre> In\u00a0[8]: Copied! <pre># process categorical variables to enter a model\ndf = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n</pre> # process categorical variables to enter a model df = pd.get_dummies(df, columns=categorical_vars, drop_first=True) In\u00a0[9]: Copied! <pre>df.head()\n</pre> df.head() Out[9]: id Age Flight Distance Inflight wifi service Departure/Arrival time convenient Ease of Online booking Gate location Food and drink Online boarding Seat comfort ... Inflight service Cleanliness Departure Delay in Minutes Arrival Delay in Minutes Gender_Male Customer Type_disloyal Customer Type of Travel_Personal Travel Class_Eco Class_Eco Plus satisfaction_satisfied 0 70172 13 460 3 4 3 1 5 3 5 ... 5 5 25 18.0 True False True False True False 1 5047 25 235 3 2 3 3 1 3 1 ... 4 1 1 6.0 True True False False False False 2 110028 26 1142 2 2 2 2 5 5 5 ... 4 5 0 0.0 False False False False False True 3 24026 25 562 2 5 5 5 2 2 2 ... 4 2 11 9.0 False False False False False False 4 119299 61 214 3 3 3 3 4 5 5 ... 3 3 0 0.0 True False False False False True <p>5 rows \u00d7 25 columns</p> In\u00a0[10]: Copied! <pre>scaler = StandardScaler()\ndf[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n</pre> scaler = StandardScaler() df[numeric_vars] = scaler.fit_transform(df[numeric_vars]) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[11]: Copied! <pre>df.to_csv(\"../data/processed_airline_passenger_satisfaction.csv\", index=False)\n</pre> df.to_csv(\"../data/processed_airline_passenger_satisfaction.csv\", index=False) In\u00a0[12]: Copied! <pre>#read csv\ndf = pd.read_csv(\"../data/processed_airline_passenger_satisfaction.csv\")\ndf.head()\n</pre> #read csv df = pd.read_csv(\"../data/processed_airline_passenger_satisfaction.csv\") df.head() Out[12]: id Age Flight Distance Inflight wifi service Departure/Arrival time convenient Ease of Online booking Gate location Food and drink Online boarding Seat comfort ... Inflight service Cleanliness Departure Delay in Minutes Arrival Delay in Minutes Gender_Male Customer Type_disloyal Customer Type of Travel_Personal Travel Class_Eco Class_Eco Plus satisfaction_satisfied 0 0.140077 -1.745279 -0.731539 0.203579 0.616172 0.173776 -1.547323 1.352264 -0.185532 1.183099 ... 1.156436 1.305870 0.266393 0.074169 True False True False True False 1 -1.598276 -0.951360 -0.957184 0.203579 -0.695245 0.173776 0.018094 -1.656326 -0.185532 -1.849315 ... 0.305848 -1.742292 -0.361375 -0.236313 True True False False False False 2 1.203935 -0.885200 -0.047584 -0.549533 -0.695245 -0.541060 -0.764614 1.352264 1.296496 1.183099 ... 0.305848 1.305870 -0.387532 -0.391554 False False False False False True 3 -1.091678 -0.951360 -0.629246 -0.549533 1.271880 1.603448 1.583511 -0.904178 -0.926545 -1.091211 ... 0.305848 -0.980251 -0.099805 -0.158692 False False False False False False 4 1.451402 1.430397 -0.978244 0.203579 -0.039537 0.173776 0.018094 0.600117 1.296496 1.183099 ... -0.544740 -0.218211 -0.387532 -0.391554 True False False False False True <p>5 rows \u00d7 25 columns</p>"},{"location":"projetos/","title":"Index","text":"<p>Este compilado de documenta\u00e7\u00f5es se refere a projetos executados em 2025.2</p>"},{"location":"projetos/classification/","title":"Projeto 1","text":""},{"location":"projetos/classification/#report-airline-passenger-satisfaction-prediction","title":"Report: Airline Passenger Satisfaction Prediction","text":""},{"location":"projetos/classification/#1-dataset-selection","title":"1. Dataset Selection","text":""},{"location":"projetos/classification/#dataset-airline-passenger-satisfaction","title":"Dataset: Airline Passenger Satisfaction","text":"<p>Source: Kaggle</p> <p>URL: https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction</p> <p>Size: 129,880 passengers x 24 features.</p>"},{"location":"projetos/classification/#reason","title":"Reason:","text":"<p>This dataset presents a highly relevant business problem\u2014predicting customer satisfaction\u2014with sufficient size and complexity (22 input features) to make an MLP model meaningful.</p>"},{"location":"projetos/classification/#2-dataset-explanation","title":"2. Dataset Explanation","text":""},{"location":"projetos/classification/#21-overview-features","title":"2.1. Overview &amp; Features","text":"<p>The dataset contains survey results from airline passengers. The goal is a binary classification: predict if a passenger is \"satisfied\" or \"neutral or dissatisfied\".</p> <p>Target Variable: satisfaction (Categorical)</p> <p>Input Features:</p> <p>Customer &amp; Travel Context: Customer Type (Loyal/Disloyal), Type of Travel, Class, Flight Distance, Delay times.</p> <p>Service Ratings (Numerical, 1-5): Key features include Online boarding, Seat comfort, Inflight wifi/service, Food and drink, and On-board service.</p>"},{"location":"projetos/classification/#22-domain-context","title":"2.2. Domain Context","text":"<p>Understanding the drivers of passenger satisfaction is critical for customer retention and revenue in the competitive airline industry. This model can directly identify key service areas for improvement.</p>"},{"location":"projetos/classification/#23-potential-issues","title":"2.3. Potential Issues","text":"<p>Class Imbalance: The target is skewed (55% \"neutral/dissatisfied\", 45% \"satisfied\").</p> <p>Missing Values: A small number of missing values exist in the Arrival Delay column.</p> <p>Outliers: Numerical features like Delay and Flight Distance may have extreme values that need handling.</p>"},{"location":"projetos/classification/#4-mlp-implementation","title":"4. MLP Implementation","text":"<p>First, we need to import the relevant libraries:</p> <ul> <li>Pandas to read the dataset</li> <li>Numpy to perform matrix operations</li> <li>MatPlotLib to generate graphs</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre> <p>Then, we need to define the important functions and derivatives: </p><pre><code># Tanh activation\ndef tanh(x):\n    return np.tanh(x)\n\n# Derivative of tanh with respect to pre-activation z\ndef tanh_derivative(z):\n    return 1.0 - np.tanh(z) ** 2\n\n# Sigmoid activation\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Binary cross-entropy loss (mean over batch/sample)\ndef binary_cross_entropy(y, y_hat):\n    eps = 1e-9\n    return -np.mean(y * np.log(y_hat + eps) + (1 - y) * np.log(1 - y_hat + eps))\n</code></pre><p></p> <p>In this case, Tanh is being used as the activation for the hidden layer, the fact that the data is normalized between -1 and 1 helps using this activation function. On the other hand, since we are dealing with a binary classification model, the output layer is using sigmoid, as a way to keep the results as 0 and 1.</p> <p>Following that, we can load the dataset and select the appropriate feature and target columns as well as define the hyparparemeters that are going to be used for the training.</p> <pre><code># Data loading and preparation\ncsv_path = \"./docs/data/processed_airline_passenger_satisfaction.csv\"\ndf = pd.read_csv(csv_path)\ntarget_col = \"satisfaction_satisfied\"\n\nif \"id\" in df.columns:\n    df = df.drop(columns=[\"id\"])\n\nfeature_cols = [c for c in df.columns if c != target_col]\n\nX = df[feature_cols].to_numpy(dtype=float)\ny = df[target_col].to_numpy()\ny = y.reshape(-1, 1)\n</code></pre> <pre><code># Hyperparameters\ninput_dim = X_train.shape[1]\nhidden_dim = 32\noutput_dim = 1\neta = 0.01\nepochs = 20\ninit_rng = np.random.RandomState(42)\n</code></pre>"},{"location":"projetos/classification/#5-model-training","title":"5. Model Training","text":"<p>Before training, we need to first split the dataset betwen train/test to allow for evaluation: </p><pre><code>rng = np.random.RandomState(42)\nperm = rng.permutation(X.shape[0])\nsplit = int(0.8 * X.shape[0])\ntrain_idx = perm[:split]\ntest_idx = perm[split:]\n\nX_train = X[train_idx]\ny_train = y[train_idx]\nX_test = X[test_idx]\ny_test = y[test_idx]\n</code></pre><p></p> <p>The parameter were initialized using an approximate method of the Xavier initialization that scales the weight with the number of inputs: </p><pre><code># Parameter initialization (Xavier for tanh)\nW1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim)\nb1 = np.zeros((hidden_dim, 1))\nW2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\nb2 = np.zeros((output_dim, 1))\n</code></pre><p></p> <p>This is the entire training script, I will then breakdown each section separately:</p> <pre><code>for epoch in range(epochs):\n    perm = rng.permutation(n_train)\n    X_shuffled = X_train[perm]\n    y_shuffled = y_train[perm]\n\n    total_loss = 0.0\n\n    for i in range(n_train):\n        x_i = X_shuffled[i].reshape(1, -1)\n        y_i = y_shuffled[i].reshape(1, 1)\n\n        Z1 = x_i.dot(W1.T) + b1.T\n        A1 = tanh(Z1)\n        Z2 = A1.dot(W2.T) + b2.T\n        A2 = sigmoid(Z2)\n\n        loss_i = binary_cross_entropy(y_i, A2)\n        total_loss += loss_i\n\n        dZ2 = A2 - y_i\n        dW2 = dZ2.T.dot(A1)\n        db2 = dZ2.T\n        dA1 = dZ2.dot(W2)\n        dZ1 = dA1 * tanh_derivative(Z1)\n        dW1 = dZ1.T.dot(x_i)\n        db1 = dZ1.T\n\n        W2 -= eta * dW2\n        b2 -= eta * db2\n        W1 -= eta * dW1\n        b1 -= eta * db1\n\n    avg_epoch_loss = total_loss / n_train\n    train_losses.append(avg_epoch_loss)\n\n    print(f\"Epoch {epoch+1}/{epochs}  Loss = {avg_epoch_loss:.6f}\")\n</code></pre>"},{"location":"projetos/classification/#51-forward-pass","title":"5.1. Forward pass","text":"<p>On the forward pass we perform 4 steps:  - Compute the weighted sum of inputs for each hidden neuron  - Apply the activation function (tanh) to the hidden layer   - Weighted sum of hidden activations for the single output neuron  - Convert the output layer into 0-1 probability (sigmoid)</p> <p>This is how those steps are performed in the code:  <code>Python Z1 = x_i.dot(W1.T) + b1.T A1 = tanh(Z1) Z2 = A1.dot(W2.T) + b2.T A2 = sigmoid(Z2)</code></p>"},{"location":"projetos/classification/#52-calculate-loss","title":"5.2. Calculate Loss","text":"<p>The loss is quickly calculated in the following code: </p><pre><code>loss_i = binary_cross_entropy(y_i, A2)\ntotal_loss += loss_i\n</code></pre> Since we are using SGD (calculating loss for each sample), the loss for each sample is added and then divided by the number of samples so we are left with the overall loss for the epoch.<p></p>"},{"location":"projetos/classification/#53-backwards-propagation","title":"5.3. Backwards Propagation","text":"<p>Now we propagate the errors backwards to allow further updating of the weights, this is performed in 6 steps:  - Output layer error  - Gradients for output layer weights and bias  - Backpropagate error into hidden layer  - Apply tanh derivative to get hidden layer error  - Gradients for hidden layer weights and bias  - Parameter updates (gradient descent)</p> <pre><code>dZ2 = A2 - y_i\ndW2 = dZ2.T.dot(A1)\ndb2 = dZ2.T\ndA1 = dZ2.dot(W2)\ndZ1 = dA1 * tanh_derivative(Z1)\ndW1 = dZ1.T.dot(x_i)\ndb1 = dZ1.T\n\nW2 -= eta * dW2\nb2 -= eta * db2\nW1 -= eta * dW1\nb1 -= eta * db1\n</code></pre> <p>At this point we have to factor in the average loss mentioned earlier: </p><pre><code>avg_epoch_loss = total_loss / n_train\ntrain_losses.append(avg_epoch_loss)\n</code></pre><p></p>"},{"location":"projetos/classification/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy","text":""},{"location":"projetos/classification/#61-data-splitting-and-validation","title":"6.1. Data Splitting and Validation","text":"<p>For this analysis, the complete dataset was divided into two distinct sets using an 80/20 ratio: 80% for the training set (<code>X_train</code>) and 20% for the test set (<code>X_test</code>).</p> <ul> <li>Training Set Size: 103,904 passengers  </li> <li>Test/Validation Set Size: 25,976 passengers  </li> </ul> <p>To ensure the reproducibility of the split, a fixed random state was utilized:</p> <pre><code>rng = np.random.RandomState(42)\n</code></pre> <p>In the absence of a separate dedicated validation set, the <code>X_test</code> set serves a dual purpose: - Used as the validation set during training for monitoring performance and guiding the early stopping mechanism. - Used as the final holdout set for evaluating the best-performing model parameters.</p> <p>This ensures the model\u2019s final evaluation is based on data it did not directly train on.</p>"},{"location":"projetos/classification/#62-training-mode","title":"6.2. Training Mode","text":"<p>The model was trained using Online Training, also known as Stochastic Gradient Descent (SGD) at the sample level.</p> <p>Rationale: The network parameters are updated after processing every single training sample (<code>(x_i, y_i)</code>). While computationally slower than batch training, this approach introduces high variance in gradient estimates, which can help the model escape shallow local minima early in training.  </p> <p>For this specific implementation, it maximizes the frequency of learning updates and clearly demonstrates the core principles of backpropagation on a single data point.</p>"},{"location":"projetos/classification/#63-overfitting-prevention-early-stopping","title":"6.3. Overfitting Prevention (Early Stopping)","text":"<p>To prevent the model from overfitting the training data\u2014a common issue when training for many epochs\u2014the Early Stopping technique was implemented.</p> <p>Logic (based on validation loss on <code>X_test</code>):</p> <ul> <li> <p>Monitoring:   The Binary Cross-Entropy Loss is calculated on the <code>X_test</code> set at the end of every epoch.</p> </li> <li> <p>Best Model Tracking:   Model parameters (<code>W\u2081</code>, <code>W\u2082</code>, <code>b\u2081</code>, <code>b\u2082</code>) are saved only if the validation loss improves (decreases) by at least   a specified minimum delta: <code>min_delta = 0.0001</code>.</p> </li> <li> <p>Patience:   A patience value of 5 epochs was set.   If the validation loss fails to meet the improvement threshold for 5 consecutive epochs, the training halts early,   and the best-saved parameters are restored for final evaluation.</p> </li> </ul> <p>This strategy ensures that the final reported metrics are derived from the model that demonstrated the best generalization performance on unseen data, rather than the model obtained at the end of the full 20 epochs.</p>"},{"location":"projetos/classification/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":""},{"location":"projetos/classification/#71-loss-curves-and-convergence-analysis","title":"7.1. Loss Curves and Convergence Analysis","text":"<p>The training process was visualized by tracking the Binary Cross-Entropy Loss for both the training and validation (test) sets across epochs.</p> <p>The trend shown in the loss curve plot (Figure 1: Loss vs. Epochs) demonstrates excellent model performance:</p> <ul> <li>Rapid Decrease: Both training and validation loss decrease rapidly within the first few epochs, confirming that the network is successfully learning the underlying patterns in the data.</li> <li>Convergence: The loss values begin to stabilize, or plateau, indicating the model is approaching a minimum in the error landscape.</li> <li>Early Stopping: The lowest validation loss (Test Loss = 0.103729) was achieved at Epoch 19. The training was halted shortly after this point due to the patience limit being reached, successfully preventing the model from continuing to learn noise in the training data, thereby avoiding overfitting and ensuring optimal generalization performance. The best model parameters were saved at this point.</li> </ul> <p>Code used to generate the loss curves:</p> <pre><code># 7. Error Curves and Visualization\nplt.figure(figsize=(10, 5))\n\n# Plot 1: Loss vs. Epochs\nplt.plot(train_losses[:len(train_losses)], label='Training Loss')\nplt.plot(test_losses[:len(test_losses)], label='Test (Validation) Loss')\nplt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Model ({best_epoch})')\nplt.title(\"Loss vs. Epochs (SGD per-sample with Early Stopping)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Binary Cross-Entropy Loss\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig('loss_curves.png') \nplt.close() \n</code></pre>"},{"location":"projetos/classification/#72-accuracy-curve","title":"7.2. Accuracy Curve","text":"<p>Although the primary early stopping decision was based on loss, tracking test accuracy further confirms the model's generalization ability, showing a steady increase and stabilization at a high level (Test Acc \u2248 0.9575 at the best epoch).</p>"},{"location":"projetos/classification/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":"<p>The final model, utilizing the weights saved by the Early Stopping procedure (at Epoch 19), was evaluated on the held-out test set (20,781 samples).</p>"},{"location":"projetos/classification/#81-performance-metrics-table","title":"8.1. Performance Metrics Table","text":"<p>The core classification metrics were calculated to provide a complete picture of the model's effectiveness.</p> <p></p> <p>Discussion on Strengths: The model demonstrates high performance across all metrics, with an overall Accuracy of 95.71%. The ROC-AUC of 0.9928 is extremely high, indicating excellent separability between the two classes (Satisfied vs. Dissatisfied) across various probability thresholds. The high Precision (96.24%) suggests that when the model predicts a passenger is \"Satisfied\", it is correct the vast majority of the time, making its positive predictions highly reliable.</p> <p>Code for calculating the final metrics:</p> <pre><code># Final Forward Pass using best parameters\nA2_test = forward_pass(X_test, W1, b1, W2, b2)\ny_pred = (A2_test &gt; 0.5).astype(int)\n\n# 8. Evaluation Metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, A2_test)\n</code></pre>"},{"location":"projetos/classification/#82-confusion-matrix-analysis","title":"8.2. Confusion Matrix Analysis","text":"<p>The confusion matrix provides a detailed breakdown of correct and incorrect predictions, offering insight into the types of errors the model makes.</p> <p>Table 1: Confusion Matrix Results</p> <p></p> <p>Interpretation: - True Positives (8,324): The model correctly identified 8,324 satisfied passengers. - False Negatives (566): The model incorrectly classified 566 truly satisfied passengers as dissatisfied. This is the primary error type, representing satisfied customers who may be mistakenly flagged for unnecessary intervention or missed retention opportunities. - False Positives (325): The model incorrectly classified 325 truly dissatisfied passengers as satisfied. This is the least frequent error type, reflecting the model's high Precision.</p> <p>The combination of high Recall (93.63%) and very high Precision (96.24%) demonstrates that the MLP model is highly reliable and accurate in predicting passenger satisfaction across the test set.</p> <p>Code used to generate the confusion matrix heatmap:</p> <pre><code># Confusion Matrix calculation\ncm = confusion_matrix(y_test, y_pred)\n\n# Plot 2: Confusion Matrix Heatmap\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Predicted 0 (Dissatisfied)', 'Predicted 1 (Satisfied)'],\n            yticklabels=['Actual 0 (Dissatisfied)', 'Actual 1 (Satisfied)'])\nplt.title(\"Confusion Matrix (Early Stop Model)\")\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\nplt.close()\n</code></pre>"},{"location":"projetos/classification/MLP/","title":"MLP","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\nimport seaborn as sns \nimport os\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score import seaborn as sns  import os In\u00a0[\u00a0]: Copied! <pre># Tanh activation\ndef tanh(x):\n    return np.tanh(x)\n</pre> # Tanh activation def tanh(x):     return np.tanh(x) In\u00a0[\u00a0]: Copied! <pre># Derivative of tanh with respect to pre-activation z\ndef tanh_derivative(z):\n    return 1.0 - np.tanh(z) ** 2\n</pre> # Derivative of tanh with respect to pre-activation z def tanh_derivative(z):     return 1.0 - np.tanh(z) ** 2 In\u00a0[\u00a0]: Copied! <pre># Sigmoid activation\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n</pre> # Sigmoid activation def sigmoid(x):     return 1 / (1 + np.exp(-x)) In\u00a0[\u00a0]: Copied! <pre># Binary cross-entropy loss (mean over batch/sample)\ndef binary_cross_entropy(y, y_hat):\n    eps = 1.0e-9\n    # Ensure inputs are in the correct format for calculation\n    y = y.flatten()\n    y_hat = y_hat.flatten()\n    \n    # Prevents log(0) and log(1-0)\n    y_hat = np.clip(y_hat, eps, 1 - eps)\n    \n    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n</pre> # Binary cross-entropy loss (mean over batch/sample) def binary_cross_entropy(y, y_hat):     eps = 1.0e-9     # Ensure inputs are in the correct format for calculation     y = y.flatten()     y_hat = y_hat.flatten()          # Prevents log(0) and log(1-0)     y_hat = np.clip(y_hat, eps, 1 - eps)          return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) <p>--- Forward Pass and Loss Functions for Testing ---</p> In\u00a0[\u00a0]: Copied! <pre># Forward pass (used to calculate loss and predictions on test set)\ndef forward_pass(X, W1, b1, W2, b2):\n    # Transposition to align with the expected format of b1 and b2 (column-vectors)\n    Z1 = X.dot(W1.T) + b1.T\n    A1 = tanh(Z1)\n    Z2 = A1.dot(W2.T) + b2.T\n    A2 = sigmoid(Z2)\n    return A2\n</pre> # Forward pass (used to calculate loss and predictions on test set) def forward_pass(X, W1, b1, W2, b2):     # Transposition to align with the expected format of b1 and b2 (column-vectors)     Z1 = X.dot(W1.T) + b1.T     A1 = tanh(Z1)     Z2 = A1.dot(W2.T) + b2.T     A2 = sigmoid(Z2)     return A2 In\u00a0[\u00a0]: Copied! <pre># Calculation of loss on the dataset (X, y)\ndef calculate_loss(X, y, W1, b1, W2, b2):\n    A2 = forward_pass(X, W1, b1, W2, b2)\n    return binary_cross_entropy(y, A2)\n</pre> # Calculation of loss on the dataset (X, y) def calculate_loss(X, y, W1, b1, W2, b2):     A2 = forward_pass(X, W1, b1, W2, b2)     return binary_cross_entropy(y, A2) In\u00a0[\u00a0]: Copied! <pre># Data loading and preparation\nbase_path = os.path.dirname(os.path.abspath(__file__))\ncsv_path = os.path.join(base_path, '..', '..', 'data', 'processed_airline_passenger_satisfaction.csv')\ntry:\n    df = pd.read_csv(csv_path)\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {csv_path}. Using dummy data.\")\n    # Fallback: Create a dummy dataset if the file is not found\n    data = {'feature1': np.random.rand(100), 'feature2': np.random.rand(100), 'satisfaction_satisfied': np.random.randint(0, 2, 100)}\n    df = pd.DataFrame(data)\n</pre> # Data loading and preparation base_path = os.path.dirname(os.path.abspath(__file__)) csv_path = os.path.join(base_path, '..', '..', 'data', 'processed_airline_passenger_satisfaction.csv') try:     df = pd.read_csv(csv_path) except FileNotFoundError:     print(f\"Error: File not found at {csv_path}. Using dummy data.\")     # Fallback: Create a dummy dataset if the file is not found     data = {'feature1': np.random.rand(100), 'feature2': np.random.rand(100), 'satisfaction_satisfied': np.random.randint(0, 2, 100)}     df = pd.DataFrame(data) In\u00a0[\u00a0]: Copied! <pre>target_col = \"satisfaction_satisfied\"\n</pre> target_col = \"satisfaction_satisfied\" In\u00a0[\u00a0]: Copied! <pre>if \"id\" in df.columns:\n    df = df.drop(columns=[\"id\"])\n</pre> if \"id\" in df.columns:     df = df.drop(columns=[\"id\"]) In\u00a0[\u00a0]: Copied! <pre>feature_cols = [c for c in df.columns if c != target_col]\n</pre> feature_cols = [c for c in df.columns if c != target_col] In\u00a0[\u00a0]: Copied! <pre>X = df[feature_cols].to_numpy(dtype=float)\ny = df[target_col].to_numpy()\ny = y.reshape(-1, 1)\n</pre> X = df[feature_cols].to_numpy(dtype=float) y = df[target_col].to_numpy() y = y.reshape(-1, 1) In\u00a0[\u00a0]: Copied! <pre>rng = np.random.RandomState(42)\nperm = rng.permutation(X.shape[0])\nsplit = int(0.8 * X.shape[0])\ntrain_idx = perm[:split]\ntest_idx = perm[split:]\n</pre> rng = np.random.RandomState(42) perm = rng.permutation(X.shape[0]) split = int(0.8 * X.shape[0]) train_idx = perm[:split] test_idx = perm[split:] In\u00a0[\u00a0]: Copied! <pre>X_train = X[train_idx]\ny_train = y[train_idx]\nX_test = X[test_idx]\ny_test = y[test_idx]\n</pre> X_train = X[train_idx] y_train = y[train_idx] X_test = X[test_idx] y_test = y[test_idx] In\u00a0[\u00a0]: Copied! <pre>print(f\"Samples: total={X.shape[0]}, train={X_train.shape[0]}, test={X_test.shape[0]}\")\nprint(f\"Features used: {len(feature_cols)}\")\n</pre> print(f\"Samples: total={X.shape[0]}, train={X_train.shape[0]}, test={X_test.shape[0]}\") print(f\"Features used: {len(feature_cols)}\") In\u00a0[\u00a0]: Copied! <pre># Hyperparameters\ninput_dim = X_train.shape[1]\nhidden_dim = 32\noutput_dim = 1\neta = 0.01\nepochs = 20\ninit_rng = np.random.RandomState(42)\n</pre> # Hyperparameters input_dim = X_train.shape[1] hidden_dim = 32 output_dim = 1 eta = 0.01 epochs = 20 init_rng = np.random.RandomState(42) In\u00a0[\u00a0]: Copied! <pre># --- Early Stopping Hyperparameters ---\npatience = 5\nmin_delta = 0.0001\nbest_loss = np.inf\npatience_counter = 0\n</pre> # --- Early Stopping Hyperparameters --- patience = 5 min_delta = 0.0001 best_loss = np.inf patience_counter = 0 In\u00a0[\u00a0]: Copied! <pre># Parameter initialization (Xavier for tanh)\nW1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim)\nb1 = np.zeros((hidden_dim, 1))\nW2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\nb2 = np.zeros((output_dim, 1))\n</pre> # Parameter initialization (Xavier for tanh) W1 = init_rng.randn(hidden_dim, input_dim) / np.sqrt(input_dim) b1 = np.zeros((hidden_dim, 1)) W2 = init_rng.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim) b2 = np.zeros((output_dim, 1)) In\u00a0[\u00a0]: Copied! <pre># --- Variables to store the BEST weights and bias (for Early Stopping) ---\nbest_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n</pre> # --- Variables to store the BEST weights and bias (for Early Stopping) --- best_W1, best_b1, best_W2, best_b2 = W1.copy(), b1.copy(), W2.copy(), b2.copy() In\u00a0[\u00a0]: Copied! <pre># Training loop using true SGD (update per sample)\nn_train = X_train.shape[0]\ntrain_losses = []\ntest_losses = [] # To store validation loss\ntest_accuracies = [] # To store validation accuracy per epoch\n</pre> # Training loop using true SGD (update per sample) n_train = X_train.shape[0] train_losses = [] test_losses = [] # To store validation loss test_accuracies = [] # To store validation accuracy per epoch In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- Starting SGD Training (with Early Stopping) ---\")\nfor epoch in range(epochs):\n    perm = rng.permutation(n_train)\n    X_shuffled = X_train[perm]\n    y_shuffled = y_train[perm]\n\n    total_train_loss = 0.0\n\n    # Training (Forward and Backward per sample)\n    for i in range(n_train):\n        x_i = X_shuffled[i].reshape(1, -1)\n        y_i = y_shuffled[i].reshape(1, 1)\n\n        # Forward Pass\n        Z1 = x_i.dot(W1.T) + b1.T\n        A1 = tanh(Z1)\n        Z2 = A1.dot(W2.T) + b2.T\n        A2 = sigmoid(Z2)\n\n        # Loss Calculation and Accumulation\n        loss_i = binary_cross_entropy(y_i, A2)\n        total_train_loss += loss_i\n\n        # Backward Pass (Gradient Calculation)\n        dZ2 = A2 - y_i \n        dW2 = dZ2.T.dot(A1)\n        db2 = dZ2.T\n        dA1 = dZ2.dot(W2)\n        dZ1 = dA1 * tanh_derivative(Z1)\n        dW1 = dZ1.T.dot(x_i)\n        db1 = dZ1.T\n\n        # Parameter Update (SGD)\n        W2 -= eta * dW2\n        b2 -= eta * db2\n        W1 -= eta * dW1\n        b1 -= eta * db1\n\n    # Epoch Loss Evaluation\n    avg_epoch_loss = total_train_loss / n_train\n    train_losses.append(avg_epoch_loss)\n\n    # Calculation of Loss and Accuracy on the Test Set (Validation)\n    A2_test_temp = forward_pass(X_test, W1, b1, W2, b2)\n    avg_test_loss = binary_cross_entropy(y_test, A2_test_temp)\n    test_losses.append(avg_test_loss)\n    \n    y_pred_test_temp = (A2_test_temp &gt; 0.5).astype(int)\n    accuracy_test_temp = accuracy_score(y_test, y_pred_test_temp)\n    test_accuracies.append(accuracy_test_temp)\n    \n    print(f\"Epoch {epoch+1}/{epochs} - Train Loss = {avg_epoch_loss:.6f} | Test Loss = {avg_test_loss:.6f} | Test Acc = {accuracy_test_temp:.4f}\")\n\n    # --- Early Stopping Check ---\n    if avg_test_loss &lt; best_loss - min_delta:\n        best_loss = avg_test_loss\n        patience_counter = 0\n        # Save the current best model weights\n        best_W1, best_b1 = W1.copy(), b1.copy()\n        best_W2, best_b2 = W2.copy(), b2.copy()\n        best_epoch = epoch + 1\n    else:\n        patience_counter += 1\n        if patience_counter &gt;= patience:\n            print(f\"\\nEarly stopping triggered after {epoch+1} epochs (Patience = {patience}). Best epoch was {best_epoch} with loss {best_loss:.6f}.\")\n            break\n</pre> print(\"\\n--- Starting SGD Training (with Early Stopping) ---\") for epoch in range(epochs):     perm = rng.permutation(n_train)     X_shuffled = X_train[perm]     y_shuffled = y_train[perm]      total_train_loss = 0.0      # Training (Forward and Backward per sample)     for i in range(n_train):         x_i = X_shuffled[i].reshape(1, -1)         y_i = y_shuffled[i].reshape(1, 1)          # Forward Pass         Z1 = x_i.dot(W1.T) + b1.T         A1 = tanh(Z1)         Z2 = A1.dot(W2.T) + b2.T         A2 = sigmoid(Z2)          # Loss Calculation and Accumulation         loss_i = binary_cross_entropy(y_i, A2)         total_train_loss += loss_i          # Backward Pass (Gradient Calculation)         dZ2 = A2 - y_i          dW2 = dZ2.T.dot(A1)         db2 = dZ2.T         dA1 = dZ2.dot(W2)         dZ1 = dA1 * tanh_derivative(Z1)         dW1 = dZ1.T.dot(x_i)         db1 = dZ1.T          # Parameter Update (SGD)         W2 -= eta * dW2         b2 -= eta * db2         W1 -= eta * dW1         b1 -= eta * db1      # Epoch Loss Evaluation     avg_epoch_loss = total_train_loss / n_train     train_losses.append(avg_epoch_loss)      # Calculation of Loss and Accuracy on the Test Set (Validation)     A2_test_temp = forward_pass(X_test, W1, b1, W2, b2)     avg_test_loss = binary_cross_entropy(y_test, A2_test_temp)     test_losses.append(avg_test_loss)          y_pred_test_temp = (A2_test_temp &gt; 0.5).astype(int)     accuracy_test_temp = accuracy_score(y_test, y_pred_test_temp)     test_accuracies.append(accuracy_test_temp)          print(f\"Epoch {epoch+1}/{epochs} - Train Loss = {avg_epoch_loss:.6f} | Test Loss = {avg_test_loss:.6f} | Test Acc = {accuracy_test_temp:.4f}\")      # --- Early Stopping Check ---     if avg_test_loss &lt; best_loss - min_delta:         best_loss = avg_test_loss         patience_counter = 0         # Save the current best model weights         best_W1, best_b1 = W1.copy(), b1.copy()         best_W2, best_b2 = W2.copy(), b2.copy()         best_epoch = epoch + 1     else:         patience_counter += 1         if patience_counter &gt;= patience:             print(f\"\\nEarly stopping triggered after {epoch+1} epochs (Patience = {patience}). Best epoch was {best_epoch} with loss {best_loss:.6f}.\")             break <p>--- End of Training and Final Evaluation ---</p> In\u00a0[\u00a0]: Copied! <pre># Use the best parameters found during training (saved before overfitting started)\nW1, b1, W2, b2 = best_W1, best_b1, best_W2, best_b2\n</pre> # Use the best parameters found during training (saved before overfitting started) W1, b1, W2, b2 = best_W1, best_b1, best_W2, best_b2 In\u00a0[\u00a0]: Copied! <pre>print(\"\\n--- Final Evaluation on the Test Set (Using Best Model) ---\")\n</pre> print(\"\\n--- Final Evaluation on the Test Set (Using Best Model) ---\") In\u00a0[\u00a0]: Copied! <pre># Final Forward Pass\nA2_test = forward_pass(X_test, W1, b1, W2, b2)\ny_pred = (A2_test &gt; 0.5).astype(int)\n</pre> # Final Forward Pass A2_test = forward_pass(X_test, W1, b1, W2, b2) y_pred = (A2_test &gt; 0.5).astype(int) In\u00a0[\u00a0]: Copied! <pre># 8. Evaluation Metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, A2_test) \n</pre> # 8. Evaluation Metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) roc_auc = roc_auc_score(y_test, A2_test)  In\u00a0[\u00a0]: Copied! <pre>print(\"-\" * 30)\nprint(f\"Final Test Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\nprint(\"-\" * 30)\n</pre> print(\"-\" * 30) print(f\"Final Test Accuracy: {accuracy:.4f}\") print(f\"Precision: {precision:.4f}\") print(f\"Recall: {recall:.4f}\") print(f\"F1-Score: {f1:.4f}\") print(f\"ROC-AUC: {roc_auc:.4f}\") print(\"-\" * 30) In\u00a0[\u00a0]: Copied! <pre># Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n</pre> # Confusion Matrix cm = confusion_matrix(y_test, y_pred) In\u00a0[\u00a0]: Copied! <pre># 7. Error Curves and Visualization\nplt.figure(figsize=(10, 5))\n</pre> # 7. Error Curves and Visualization plt.figure(figsize=(10, 5)) In\u00a0[\u00a0]: Copied! <pre># Plot 1: Loss vs. Epochs\n# We only plot up to the last executed epoch\nplt.plot(train_losses[:len(train_losses)], label='Training Loss')\nplt.plot(test_losses[:len(test_losses)], label='Test (Validation) Loss')\nplt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Model ({best_epoch})')\nplt.title(\"Loss vs. Epochs (SGD per-sample with Early Stopping)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Binary Cross-Entropy Loss\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig('loss_curves.png') \nplt.close() \n</pre> # Plot 1: Loss vs. Epochs # We only plot up to the last executed epoch plt.plot(train_losses[:len(train_losses)], label='Training Loss') plt.plot(test_losses[:len(test_losses)], label='Test (Validation) Loss') plt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Model ({best_epoch})') plt.title(\"Loss vs. Epochs (SGD per-sample with Early Stopping)\") plt.xlabel(\"Epoch\") plt.ylabel(\"Binary Cross-Entropy Loss\") plt.grid(True) plt.legend() plt.tight_layout() plt.savefig('loss_curves.png')  plt.close()  In\u00a0[\u00a0]: Copied! <pre># Plot 2: Confusion Matrix Heatmap\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Predicted 0 (Dissatisfied)', 'Predicted 1 (Satisfied)'],\n            yticklabels=['Actual 0 (Dissatisfied)', 'Actual 1 (Satisfied)'])\nplt.title(\"Confusion Matrix (Early Stop Model)\")\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\nplt.close()\n</pre> # Plot 2: Confusion Matrix Heatmap plt.figure(figsize=(6, 5)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,             xticklabels=['Predicted 0 (Dissatisfied)', 'Predicted 1 (Satisfied)'],             yticklabels=['Actual 0 (Dissatisfied)', 'Actual 1 (Satisfied)']) plt.title(\"Confusion Matrix (Early Stop Model)\") plt.ylabel(\"True Label\") plt.xlabel(\"Predicted Label\") plt.tight_layout() plt.savefig('confusion_matrix.png') plt.close() In\u00a0[\u00a0]: Copied! <pre># Plot 3: Final Metrics Table\nmetrics = {\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n    'Value': [f\"{accuracy:.4f}\", f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", f\"{roc_auc:.4f}\"]\n}\nmetrics_df = pd.DataFrame(metrics)\n</pre> # Plot 3: Final Metrics Table metrics = {     'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],     'Value': [f\"{accuracy:.4f}\", f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", f\"{roc_auc:.4f}\"] } metrics_df = pd.DataFrame(metrics) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(6, 2)) # Size adjustment for the table\nax.axis('off') # Removes axes\nax.axis('tight')\ntable = ax.table(cellText=metrics_df.values,\n                     colLabels=metrics_df.columns,\n                     cellLoc = 'center', \n                     loc = 'center',\n                     colColours=['#f3f3f3']*2)\ntable.auto_set_font_size(False)\ntable.set_fontsize(12)\ntable.scale(1, 1.5)\nplt.title(\"Final Classification Metrics (Early Stop Model)\")\nplt.savefig('metrics_table.png', bbox_inches='tight', pad_inches=0.1)\nplt.close()\n</pre> fig, ax = plt.subplots(figsize=(6, 2)) # Size adjustment for the table ax.axis('off') # Removes axes ax.axis('tight') table = ax.table(cellText=metrics_df.values,                      colLabels=metrics_df.columns,                      cellLoc = 'center',                       loc = 'center',                      colColours=['#f3f3f3']*2) table.auto_set_font_size(False) table.set_fontsize(12) table.scale(1, 1.5) plt.title(\"Final Classification Metrics (Early Stop Model)\") plt.savefig('metrics_table.png', bbox_inches='tight', pad_inches=0.1) plt.close() In\u00a0[\u00a0]: Copied! <pre>print(\"Plots saved (loss_curves.png, confusion_matrix.png, and metrics_table.png).\")\n</pre> print(\"Plots saved (loss_curves.png, confusion_matrix.png, and metrics_table.png).\")"},{"location":"thisdocumentation/","title":"This documentation","text":""},{"location":"thisdocumentation/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}